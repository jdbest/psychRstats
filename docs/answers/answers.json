[
  {
    "path": "answers/10-lab/",
    "title": "Chi-square and factorial ANOVA (Lab 12) Exercises, Completed",
    "description": "Completed exercises for the twelfth lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-11-20",
    "categories": [
      "rlanguage",
      "chi-squared",
      "ANOVA",
      "dplyr",
      "effect-size"
    ],
    "contents": "\n\n\n\nThis document is meant to be used to practice after you have completed the tutorial for today’s lab.\nIf you intend to work on these exercises while referring to the tutorial, there are instructions on the wiki on how to do so. You may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nImportant reminder: as with every time you open RStudio, don’t forget to load the libraries, below.\nObjectives\nIn the tutorial, we learned about using lm() and anova() for factorial ANOVA, about plotting bars against one another using geom_col() with position = position_dodge() in the parentheses, and about using the table() function with chisq.test(). In the exercises, you’ll use those to practice one of each kind of test.\nAs always, a version of these exercises with my answers will be posted at the end of the week to the lab website: https://faculty.bard.edu/~jdainerbest/psy-203/labslist.html\nDon’t forget to (a) save and (b) knit the document frequently, so you’ll keep track of your work and also know where you run into errors.\nLoading packages\nYou must load packages if you intend to use their functions. Run the following code chunk to load necessary packages for these exercises.\n\n\nlibrary(tidyverse)\n\nToday’s work\nChi-squared test of independence\nImagine that we get some data on rolling a six-sided die 150 times—just run the chunk below (run all of the lines at once). You’ll have 150 numbers between 1 and 6, representing your throwing a die over and over again.\n\n\n# run this whole chunk:\nset.seed(42)\ndicerolls <- data.frame(\n  rolls = sample(c(1:6), 150, replace = TRUE, prob = c(.3,.1,.1,.1,.1,.3))\n)\n\nRun a chi-squared test to test if the die is fair. Take a look at the data, first. Is it tabulated in the right way? What would a p-value less than .05 mean here?\n\n\nchisq.test(table(dicerolls))\n\n    Chi-squared test for given probabilities\n\ndata:  table(dicerolls)\nX-squared = 26, df = 5, p-value = 8.924e-05\n\nTake a look at the code that “tossed” the die 150 times. Pull the probabilities from there (under the prob argument) and see if when you set that as your p in the chi-squared test, it changes the results. Why // why not?\n\n\nchisq.test(table(dicerolls), p = c(.3,.1,.1,.1,.1,.3))\n\n    Chi-squared test for given probabilities\n\ndata:  table(dicerolls)\nX-squared = 8.1333, df = 5, p-value = 0.149\n\nYou can also plot the coin tosses. Use the ggplot() function on the data after you’ve run group_by() (grouping by the one column in it) and %>% to summarize() to get the n() values (i.e., get the frequencies, and then use ggplot().)\n\n\ndicerolls %>%\n  group_by(rolls) %>%\n  summarize(n = n()) %>%\n  ggplot(aes(x = rolls, y = n)) +\n  geom_col() +\n  scale_x_continuous(breaks = 1:6) +\n  theme_minimal() +\n  labs(x = \"Side of die\", y = \"Number of rolls\")\n\n\n# you could also use the `barplot()` function on the `table()`:\nbarplot(table(dicerolls))\n\n\nChi-squared test of goodness of fit\nThe data below is a “contingency table” (you don’t need to run table() since it’s already one!). It contains 13 house-tasks and their distribution in heterosexual couples: rows are the different tasks; values are the frequencies of the tasks done: by the wife only, by the husband only, by alternating partners, or together. Run the whole code chunk below:\n\n\ncontingency.table <- structure(list(Wife = c(156L, 124L, 77L, 82L, 53L, 32L, 33L, \n12L, 10L, 13L, 8L, 0L, 0L), Alternating = c(14L, 20L, 11L, 36L, \n11L, 24L, 23L, 46L, 51L, 13L, 1L, 3L, 1L), Husband = c(2L, 5L, \n7L, 15L, 1L, 4L, 9L, 23L, 75L, 21L, 53L, 160L, 6L), Jointly = c(4L, \n4L, 13L, 7L, 57L, 53L, 55L, 15L, 3L, 66L, 77L, 2L, 153L)), class = \"data.frame\", row.names = c(\"Laundry\", \"Main_meal\", \"Dinner\", \"Breakfeast\", \"Tidying\", \"Dishes\", \"Shopping\",  \"Official\", \"Driving\", \"Finances\", \"Insurance\", \"Repairs\", \"Holidays\"))\nchores <- structure(list(chore = structure(c(1L, 1L, 1L, 1L, 2L, 2L, 2L, \n2L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, \n6L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 10L, 10L, \n10L, 10L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, \n13L), .Label = c(\"Laundry\", \"Main_meal\", \"Dinner\", \"Breakfeast\", \n\"Tidying\", \"Dishes\", \"Shopping\", \"Official\", \"Driving\", \"Finances\", \n\"Insurance\", \"Repairs\", \"Holidays\"), class = \"factor\"), who = c(\"Wife\", \n\"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \n\"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \n\"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \n\"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \n\"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \n\"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \n\"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \n\"Jointly\", \"Wife\", \"Husband\", \"Alternating\", \"Jointly\", \"Wife\", \n\"Husband\", \"Alternating\", \"Jointly\"), number = c(156L, 2L, 14L, \n4L, 124L, 5L, 20L, 4L, 77L, 7L, 11L, 13L, 82L, 15L, 36L, 7L, \n53L, 1L, 11L, 57L, 32L, 4L, 24L, 53L, 33L, 9L, 23L, 55L, 12L, \n23L, 46L, 15L, 10L, 75L, 51L, 3L, 13L, 21L, 13L, 66L, 8L, 53L, \n1L, 77L, 0L, 160L, 3L, 2L, 0L, 6L, 1L, 153L)), row.names = c(NA, \n-52L), class = c(\"tbl_df\", \"tbl\", \"data.frame\"))\n\nLet’s plot the data from chores, first. This is not the contingency table—take a look.\nUse ggplot() to graph chores, with x being who does which chore, y being the number of people who responded which way, and fill being the chore itself.\n\n\nchores %>%\n  ggplot(aes(x = who, y = number, fill = chore)) +\n  geom_col(position = position_dodge()) +\n  theme_minimal() +\n  labs(x = \"Who Does the Chore\", y = \"How Many Couples\", fill = \"Chore\")\n\n\nMake some informed guesses: in this data, are chores done evenly or unevenly?\n\nSure looks uneven… many chores are clearly gendered.\n\nUse the chisq.test() function to test your hypothesis based on the contingency.table … table.\n\n\nchisq.test(contingency.table)\n\n    Pearson's Chi-squared test\n\ndata:  contingency.table\nX-squared = 1944.5, df = 36, p-value < 2.2e-16\n\nWrite up the results of the test, being sure to describe whether it shows independence or not.\n\n\n# per actor\npercentages <- contingency.table %>% \n    summarize(percentages = 100 * round(colSums(.)/sum(.), 3)) # get percentages\nrownames(percentages) <- names(contingency.table)\n# percentages\n\n# per activity:\nround(100 * (contingency.table / rowSums(contingency.table)), 1)\n\n           Wife Alternating Husband Jointly\nLaundry    88.6         8.0     1.1     2.3\nMain_meal  81.0        13.1     3.3     2.6\nDinner     71.3        10.2     6.5    12.0\nBreakfeast 58.6        25.7    10.7     5.0\nTidying    43.4         9.0     0.8    46.7\nDishes     28.3        21.2     3.5    46.9\nShopping   27.5        19.2     7.5    45.8\nOfficial   12.5        47.9    24.0    15.6\nDriving     7.2        36.7    54.0     2.2\nFinances   11.5        11.5    18.6    58.4\nInsurance   5.8         0.7    38.1    55.4\nRepairs     0.0         1.8    97.0     1.2\nHolidays    0.0         0.6     3.8    95.6\n\n\nA chi-square test shows that chores were not performed independent of who performs it, \\(\\chi^2(36)=1944.5,p<.05\\). Wives did 34.4% of the chores, 29.2% were done jointly, and 21.8% were done by the husband. The remaining 14.6% of the chores were done on an alternating schedule. Some activities were almost entirely done by the wife (e.g., laundry, 88.6% done by the wife, or preparing the main meal, 81%) and others almost entirely done by the husband (repairs, 97%). Driving (36.7%) and official business (47.9%) were the most likely to alternate; holidays (95.6%) were the most likely to be handles jointly.\n\nSimulating p-values\nGenerally speaking, chi-squared tests are actually quite bad at getting a p-value. One way that R gets around this is by “simulating” p-values—running random selections of the data and considering whether that would be likely to happen by chance. This is called the Monte Carlo method.\nYou can try it with any of your chi-squared tests by adding two arguments: setting simulate.p.value equal to TRUE, and choosing how many replications, B, you want, by setting B equal to some number (I recommend 10000).\nDo that with the chisq.test() for the contingency.table.\n\n\nchisq.test(contingency.table, simulate.p.value = TRUE, B = 10000)\n\n    Pearson's Chi-squared test with simulated p-value (based on\n    10000 replicates)\n\ndata:  contingency.table\nX-squared = 1944.5, df = NA, p-value = 9.999e-05\n\nHow does the results from the test change?\n\nThe value for \\(\\chi^2\\) didn’t change, but the p-value got a lot closer to \\(p=.05\\)—it’s \\(p=9.9\\times{}10^{-5}\\) instead of \\(p=2.2\\times{}10^{-16}\\) now.\n\n(For the record, this is why we didn’t get into the pchisq() function.)\nFactorial ANOVA\nLet’s just do this based on the penguins data since we know it already. Load it by running the following chunk:\n\n\nlibrary(palmerpenguins)\ndata(penguins)\n\nDrop the penguins for whom sex is not known, using filter() and ! is.na(). Be sure to save the correctly filtered data back to penguins; you’ll now have 333 rows instead of 344.\n\n\npenguins <- penguins %>%\n  filter(! is.na(sex))\n\nIs there an interaction between penguin sex and the island they live on in predicting body_mass_g? Use lm() and then anova() on the model.\n\n\nmodel.p <- lm(body_mass_g ~ sex * island, data = penguins)\nanova(model.p)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n            Df   Sum Sq  Mean Sq F value Pr(>F)    \nsex          1 38878897 38878897 137.264 <2e-16 ***\nisland       2 82697352 41348676 145.984 <2e-16 ***\nsex:island   2  1063288   531644   1.877 0.1547    \nResiduals  327 92620129   283242                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWrite up the results.\n\nThere was no interaction between sex and island, \\(F(2, 327)=1.88,p=.15\\), but there was a main effect of sex, \\(F(1, 327)=137.26,p<.05\\) and a main effect of island, \\(F(2,327)=145.98,p<.05\\).\n\nGraph the results. Use both a geom_col() and a geom_point() + geom_line(). For both, be sure to find the means before graphing. Do also include a theme and labels for the axes.\nFor geom_col(): 1. Add on error bars using the SEM. 2. Don’t forget to include position = position_dodge() 3. In the aes(), make sure to set fill equal to one of your independent variables. 4. For the geom_errorbar(), add the following code so they line up with the bars (but not inside the aes(): position = position_dodge(.9))\n\n\npenguin_means <- penguins %>%\n  group_by(island, sex) %>%\n  summarize(mean_weight = mean(body_mass_g),\n            sem_weight  = sd(body_mass_g) / sqrt(n()),\n            .groups = \"drop_last\")\n\nggplot(penguin_means, \n       aes(x = island, y = mean_weight, fill = sex)) +\n  geom_col(position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean_weight - sem_weight,\n                    ymax = mean_weight + sem_weight),\n                width = .4, position = position_dodge(.9)) +\n  theme_classic() +\n  labs(x = \"Island\", y = \"Mean weight (g)\", fill = \"Sex\")\n\n\nFor the combination of geom_line() and geom_point(): 1. In the main aes() tag, set color equal to one of your independent variables, instead of fill. Also set group equal to the same variable—this will make the lines connect the correct points. 2. Add on error bars using the SEM—almost identical to the geom_col(), but without the position = position_dodge() bit.\n\n\nggplot(penguin_means, \n       aes(x = island, y = mean_weight, color = sex, group = sex)) +\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean_weight - sem_weight,\n                    ymax = mean_weight + sem_weight),\n                width = .4) +\n  theme_classic() +\n  labs(x = \"Island\", y = \"Mean weight (g)\", fill = \"Sex\")\n\n\nWhich of these better describes the data?\n\nBoth are useful!\n\nHave any feedback about the exercises? Let me know at the exit survey and select Lab 12.\n\n\n",
    "preview": "answers/10-lab/chisq-and-factorial-ANOVA-lab-12-completed_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-11-19T16:24:40-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/12-test-yourself-II/",
    "title": "Final Lab Project",
    "description": "PSY 203: Group final project",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-11-18",
    "categories": [
      "practice",
      "rlanguage"
    ],
    "contents": "\nTable of Contents\nObjectives\nDeadlines \nRequirements \nThe data \nYour group \nHypotheses, Preregistration, and Introduction\nAnalyses\nBrief discussion\nGroup contributions \n\nThe documents\nGrading\nWhat does “all requirements” entail?\nHow do we get the rest of the points?\n\nPreregistration template\nThe primary website for this course is https://faculty.bard.edu/~jdainerbest/psy-203\nThese are all of the instructions for the final lab project. The list of labs is here.\nThis website contains R lab code for labs in Bard College’s Fall 2020 for Statistics for Psychology, taught by Prof. Justin Dainer-Best.\nThe project is described in the syllabus as follows: In your group project, you will perform a data analysis on real data, using the skills you’ve developed in the labs. This group project is a semester-summarizing version of the solo project—you will develop research questions, create visualizations, carry out analyses, and produce a final document that reports all of them.\nThe project is worth 40 points; completing all requirements and turning the project in on-time will result in the receipt of at least 32.\nYou may use any resources you choose in this (e.g., textbook, web searches, stats study rooms), although of course the work must be your own groups’ work. Adapting others’ code (e.g., from a post you find online) is great! Using someone else’s code for the exact data you have is not. Ask me if you have any questions.\nObjectives\nAs with the solo project, the goals are:\nTo help you to practice the skills learned in lab\nTo help you understand what happens in the data analysis and reporting sections of a research project\nTo give you functional skills from the use of R and RStudio\nTo learn the process behind data anlysis in published research\nDon’t forget to (a) save and (b) knit the document frequently, so you’ll keep track of your work and also know where you run into errors.\nDeadlines \nThe final project is due on Brightspace on December 14, 2020\nYou should plan to also meet the following deadlines (more info below):\nYou should read this document, in full, as soon as possible\nBy 11/30, I should know who is in your group, and which dataset (below) your group is using (send me an email CCing everyone in the group)\nBy 12/7, you should have asked me any major questions—e.g., “is this analysis appropriate?”—and submitted a preregistration to me via email (again, one email, copying everyone in your group)\nAgain, the final document is due 12/14\nRequirements \nThe data \nYou get to choose your dataset! You must choose one of the following datasets (or another one you let me know about), and read about it. (If you’d like to find your own, you might try looking at https://www.icpsr.umich.edu/web/pages/ICPSR/index.html or https://dataverse.harvard.edu/ to find them.)\nHere are the datasets I suggest:\nClimate Change in the American Mind: National Survey Data on Public Opinion (2008-2017): https://osf.io/w36gn/\nCorrelates of War: https://correlatesofwar.org/data-sets\nWorld Values Survey: http://www.worldvaluessurvey.org/WVSContents.jsp\nPro Publica data on criminal justice: https://www.propublica.org/datastore/datasets/criminal-justice\nPro Publica data on health: https://www.propublica.org/datastore/datasets/health\nFBI Hate Crime Reports: https://github.com/emorisse/FBI-Hate-Crime-Statistics/tree/master/2013\nWhichever data you decide to use, you should (a) read about it in detail, (b) plan to cite the data in APA style, and (c) make sure that you can read the data into R. You can message me with questions on that topic. You may need to do some data processing. Any processing that you do before importing it into R (e.g., opening it up in Google Sheets or Excel and renaming columns) should be reported in your final document.\nSome of these datasets may require more processing than others. Please don’t hesitate to ask questions if so, but don’t leave this till the last minute.\nYour group \nYour group should be 3–4 members of the class, ideally all in your same lab section so that you may work together during the project work day.\nHypotheses, Preregistration, and Introduction\nYou should plan to frame testable hypotheses which involve some of the variables described in your dataset. Those should include both a research hypothesis (\\(H_1\\)) and a null hypothesis (\\(H_0\\)).\nYour statistical framing of the hypotheses (involving means) should assume two-tailed tests. However, your preregistration should also suggest the direction you anticipate. For example, will a correlation be positive or negative? Which group will have higher scores? Will the chi-squared test find independence? All tests (described further immediately below this section) should have a preregistered hypothesis.\nPlease refer to the bottom of this document for an abbreviated preregistration template.\nIn the final document, you should also include a brief introduction which describes the data and cites it, reports your directional hypotheses, and explains why you’ve made those hypotheses.\nAnalyses\nStatistical tests\nYou should use multiple statistical tests that provide evidence for or against your hypotheses. At minimum, you must conduct tests that fall into three of the following groups: (1) regression or correlation, (2) one-way or factorial ANOVA, (3) independent-samples or decedent-samples t-tests, (4) chi-squared test.\nIf you’re assigning the results of a test to a variable (e.g., model <- lm(DV ~ IV1 * IV2, data = data)), then you must also print the model (so that I can see what it looks like):\n\nmodel <- lm(DV ~ IV1 * IV2, data = data)\nmodel\nanova(model)\nAll tests must be interpreted immediately (in the document) after the code, with the results printed in the final document. Your interpretation should always include whether the test is significant or not. It frequently will be useful to include the group means. As always, you should generally round to two significant digits after the decimal.\nPlots \nYou should create at least four plots (figures). At least two of those should involve some sort of comparison between variables—i.e., not simply be histograms or boxplots. Several plots should help you interpret the results of the tests. The plots should be “in-line,” meaning that they follow the relevant section. For example, a bar graph might be useful immediately following your running a factorial ANOVA.\nBrief discussion\nAfter all tests, you should include a “discussion section” which explains what your tests found and why it matters (or does not). Null results are absolutely fine!\nGroup contributions \nAt the bottom of your document, you should list all group members. Next to each member’s name, detail which of the following roles they played, out of the following:\nChose dataset\nDeveloped hypotheses\nWrote preregistration\nWrote introduction\nWrote code for analyses\nCreated graphs\nReported results of analyses\nWrote discussion\nEdited code\nEdited document\nFor example (although more overlap may be better!):\nFaux Auteur: Chose dataset, Developed hypotheses, Wrote preregistration, Wrote introduction, Wrote discussion, Edited document\nErsatz Autor: Chose dataset, Developed hypotheses, Created graphs, Edited code\nFalso Autor: Wrote code for analyses, Reported results of analyses, Wrote discussion, Edited document\nThe documents\nFor both the preregistration and the final document, you should plan to create a new R Markdown document for your final project. (Yes, you have to use R Markdown; you may not do this in a Word doc—I want to be able to see the code and the image it produces.) Your document should include the following “code” at the top (replacing what is automatically generated), with your title after “title” and your names under the - marks following “author”. You should knit that document.\n\n---\ntitle: \nauthor: \n  - First name here\n  - Second name here\n  - etc.\ndate: \noutput: \n  html_document:\n    self_contained: yes\n---\nGrading\nThe project is worth 40 points; completing all requirements and turning the project in on-time will result in the receipt of at least 32 of them.\nWhat does “all requirements” entail?\nTurning in the preregistration on-time with three or more hypotheses\nTurning in a logical and readable knitted R Markdown document (you won’t lose many points for only turning in an Rmd file with the data, but you will lose some—take time to make sure it knits)\nIncluding the following (complete) sections of the final document: introduction, analyses, and brief discussion\nIncluding 3+ tests (based on the hypotheses, and from different “families” of tests)\nIncluding 4+ plots with labeled axes\nProperly citing the data (and any relevant external sources)\nIncluding the group members’ contributions at the bottom\nMissing any of these will result in fewer points. However, doing something wrong (e.g., misinterpreting the results of a test) will not necessarily result in lost points.\nHow do we get the rest of the points?\nMore writing does not mean better—a good project can be brief.\nComplete the self-grading rubric\nMake sure that your code “works” and is correctly interpreted\nWork together—look over one another’s work, and make sure that everything makes sense\nMake plots that look good, have thoroughly labeled axes and titles, and clearly show the results of the test in question\nThoughtfully reflect on the tests and their results in the discussion\nThe remaining points will be awarded as follows:\n\n\nPoints\n\n\nTopic\n\n\n2\n\n\nClever question and tests\n\n\n2\n\n\nExcellent interpretation of results\n\n\n2\n\n\nPlots are complete and explain information\n\n\n2\n\n\nThere is a clear ‘message’ to the project: hypotheses link to one another and to tests\n\n\nIt is possible, although unlikely, that different students in the same group will receive different scores.\nPreregistration template\nUse the following for your preregistration in a new R Markdown document. Note that the language under the headers (i.e., the bits that don’t start with a #) should be deleted—they’re just explaining the section.\n# Variables\n## Independent Variables What are your independent / grouping / predictor variables (including mediators and moderators) ? Explain how you operationalize each variable\n## Dependent Variables What are your dependent / outcome variables? Explain how you operationalize each variable.\n# Hypotheses What are your primary study hypotheses / research questions?\n# Sampling What is the sample size?\n## Sample characteristics Who is the sample representing?\n# Analysis plan\n## Significance threshold What will be your criterion for determining statistical significance?\n## Exclusion criteria Will you exclude participants from data analysis based on any of the reasons listed below? Failed attention check; Failed manipulation check; Missing data\n## Outliers What criterion (if any) will you use to determine whether a participant is an outlier?\n## Statistical tests Which statistical tests will you use to conduct your data analyses? ANOVA; Correlation; t-test; Chi-square; Regression; Other/Additional\nIf relevant, describe what types of follow-up tests will you perform (e.g., post-hoc; simple main effects). If you will conduct planned comparisons, explain the nature of those comparisons\n\n\n",
    "preview": {},
    "last_modified": "2020-11-19T16:25:50-05:00",
    "input_file": {}
  },
  {
    "path": "answers/09-lab/",
    "title": "Correlation and Regression (Lab 10) Exercises, Completed",
    "description": "Completed exercises for the tenth lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-11-06",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "correlation",
      "regression",
      "real-data",
      "effect-sizes"
    ],
    "contents": "\n\n\n\nThis document is meant to be used to practice after you have completed the tutorial for today’s lab. Make sure to put your name as the author of the document, above!\nIf you intend to work on these exercises while referring to the tutorial, there are instructions on the wiki on how to do so. You may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nImportant reminder: as with every time you open RStudio, don’t forget to load the libraries, below.\nObjectives\nIn the tutorial, we learned about using lm() and summary() for regressions, and cor() and cor.test() for correlations. You’ll use those and the library(ggplot2) functions to plot them to make further sense of the predictions data, including adding regression lines. You’ll also practice (briefly) filter() and a few other functions to clean up the data as provided.\nAs always, a version of these exercises with my answers will be posted at the end of the week to the lab website: https://faculty.bard.edu/~jdainerbest/psy-203/labslist.html\nDon’t forget to (a) save and (b) knit the document frequently, so you’ll keep track of your work and also know where you run into errors.\nLoading packages\nAs always, you must load packages if you intend to use their functions. Run the following code chunk to load necessary packages for these exercises.\n\n\nlibrary(tidyverse)\n\nImporting data\nAs discussed in the tutorial, we’re using data from Beall, Hofer, & Shaller (2016).\n\nBeall, A. T., Hofer, M. K., & Shaller, M. (2016). Infections and elections: Did an Ebola outbreak influence the 2014 U.S. federal elections (and if so, how)? Psychological Science, 27, 595-605. https://doi.org/10.1177/0956797616628861\n\nMake sure you read the description of the study in the tutorial—it’s important for thinking about what we’re doing in these exercises.\nIn the tutorial, we used a “cleaned-up” version of the data. (It’s in a folder called /www.) But let’s actually use the raw data here: that one is called beall_untidy.csv and should be in the same folder as this document.\nThe data was downloaded with this file. Load it using the read_csv() command—probably with the code below:\n\n\npredictions <- read_csv(\"beall_untidy.csv\")\n\nToday’s task\nFor the questions below, create your own code chunks and insert all code into them.\nBe sure to assign the resulting data to itself or to a new data frame, so you can use it in the subsequent questions. Filter the data using the filter() function:\nRemove the two lines at the very top for which there are NAS even in the Date and Month column\nRemove the column DJIA with either select() (putting a - in front of the name will remove it) or by assigning predictions$DJIA to the value NULL\nYou should now have 65 observations and 14 variables in the Environment.\n\n\n\npredictions <- tibble(predictions) %>%\n  filter( ! is.na(Month) ) %>%\n  select(-DJIA)\n\nThe authors report that “Across all days in the data set, [the Ebola-search-volume index] was very highly correlated with an index—computed from LexisNexis data—of the mean number of daily news stories about Ebola during the preceding week, r = .83, p < .001.” Calculate this correlation yourself using cor.test() and your predictions data. (You’ll use the columns Ebola.Search.Volume.Index and LexisNexisNewsVolumeWeek) Then, briefly report the correlation. Is it significant?\n\n\ncor.test(predictions$Ebola.Search.Volume.Index, predictions$LexisNexisNewsVolumeWeek)\n\n    Pearson's product-moment correlation\n\ndata:  predictions$Ebola.Search.Volume.Index and predictions$LexisNexisNewsVolumeWeek\nt = 11.759, df = 63, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7331684 0.8923563\nsample estimates:\n      cor \n0.8288528 \n\n\nThere was a significant relationship between the Ebola-search-volume index and the LexisNexis index, \\(r(63)=.83, 95% CI [.73, .89], p < .05\\)\n\nPlot that relationship using ggplot() + geom_point(). Add a theme and label the axes. Add a regression line using geom_smooth() or geom_abline() (you’ll get the data in the next question).\n\n\nggplot(predictions, aes(x = Ebola.Search.Volume.Index, \n                        y = LexisNexisNewsVolumeWeek)) +\n  geom_point() +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\") +\n  labs(x = \"Ebola-search-volume index\", y = \"LexisNexis index\")\n\n\nUse the lm() function to create a regression model of the same relationship. Then use summary() to get the results. Report them succinctly below. Also report what parallels exist between the numbers from this regression and the correlation.\n\n\nmodel <- lm(Ebola.Search.Volume.Index ~ LexisNexisNewsVolumeWeek, \n            data = predictions)\nsummary(model)\n\nCall:\nlm(formula = Ebola.Search.Volume.Index ~ LexisNexisNewsVolumeWeek, \n    data = predictions)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.615  -7.050  -1.244   9.823  24.349 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              -1.91116    2.73372  -0.699    0.487    \nLexisNexisNewsVolumeWeek  0.15516    0.01319  11.759   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.88 on 63 degrees of freedom\nMultiple R-squared:  0.687, Adjusted R-squared:  0.682 \nF-statistic: 138.3 on 1 and 63 DF,  p-value: < 2.2e-16\n\n\nThere was a statistically-significant relationship between the two indexes, \\(b=0.16,p<.05\\), with an \\(R^2\\) of .69, \\(p<.05\\).\n\nUse filter() to select only the scores from the two-week period including the last week of September and the first week of October. You could look at the Month and Date columns… but the third column might be more helpful. Don’t forget to assign this to a new data frame so we can use it.\n\n\nhighanxtime <- filter(predictions, Two.weeks.prior.to.outbreak.only==1)\n\nOn the full dataset, run the correlation analyses we did in the tutorial, for the association between Ebola search volume index and voter intention index.\nWith the filtered data from #5, re-run the correlation analyses for the association between Ebola search volume index and voter intention index. Is the correlation higher or lower?\n\n\ncor.test(highanxtime$Voter.Intention.Index, \n         highanxtime$Ebola.Search.Volume.Index)\n\n    Pearson's product-moment correlation\n\ndata:  highanxtime$Voter.Intention.Index and highanxtime$Ebola.Search.Volume.Index\nt = 15.975, df = 6, p-value = 3.821e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9351079 0.9979890\nsample estimates:\n      cor \n0.9884478 \n\n\nIt’s much higher—although note that there are many fewer data points!\n\nPlot that filtered data. Add a regression line. Label the axes and add a theme.\n\n\nggplot(highanxtime, \n       aes(x = Ebola.Search.Volume.Index, y = Voter.Intention.Index)) +\n  geom_point() +\n  theme_classic() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\") +\n  labs(x = \"Ebola-search-volume index\", y = \"LexisNexis index\")\n\n\nHave any feedback about the exercises? Let me know at the exit survey and select Lab 9.\n\n\n",
    "preview": "answers/09-lab/correg-lab-10-completed_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-11-19T16:23:58-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/08-lab/",
    "title": "One-way ANOVA (Lab 09) Exercises, Completed",
    "description": "Completed exercises for the ninth lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "ANOVA",
      "real-data",
      "effect-sizes"
    ],
    "contents": "\n\n\n\nThis document is meant to be used to practice after you have completed the tutorial for today’s lab. Make sure to put your name as the author of the document, above!\nIf you intend to work on these exercises while referring to the tutorial, there are instructions on the wiki on how to do so. You may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nImportant reminder: as with every time you open RStudio, don’t forget to load the libraries, below.\nObjectives\nIn the tutorial, we talked about how to run ANOVAs “manually” and how to do them using R’s built-in functions. Now, you’ll practice doing so in R, and learn to use ggplot2 functions to plot it.\nAs always, a version of these exercises with my answers will be posted at the end of the week to the lab website: https://faculty.bard.edu/~jdainerbest/psy-203/labslist.html\nI encourage you to do two things as you work through this document: (1) Save it frequently! Hit the disk image above to save, hit Command/Ctrl and s, or go to the File menu and click save. When the document is saved, the title of it will go from red to black. (2) Practice Knitting the document by hitting the Knit button. You can do it now—it’ll create an HTML file in the folder where this file lives. Knitting will both help to identify problems and provide a complete document for me to review.\nIf you would like feedback on your answers, you may also upload the final version of this document (knitted, ideally) to Brightspace.\nLoading packages\nAs always, you must load packages if you intend to use their functions. (“Turn on the lights.”) Run the following code chunk to load necessary packages for these exercises.\n\n\nlibrary(tidyverse)\n# tidyverse loads these that we'll use today: (you don't need to load them individually)\n# library(readr)\n# library(dplyr)\n# library(ggplot2)\n# library(tidyr)\n\nImporting data\nAs discussed in the tutorial, we’re using data from James and colleagues (2015).\n\nJames, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J. R., Milton, A. L., & Holmes, E. A. (2015). Computer game play reduces intrusive memories of experimental trauma via reconsolidation-update mechanisms. Psychological Science, 26, 1201–1215. https://doi.org/10.1177%2F0956797615583071\n\nMake sure you read the description of the study in the tutorial—it’s important for thinking about what we’re doing in these exercises.\nThe data was downloaded with this file. Load it using the read_csv() command:\n\n\ntetris <- read.csv(\"data/James.csv\")\n\nToday’s task\nFor the questions below, create your own code chunks (how?: instructions here) and insert all code into them.\nWe spent the tutorial talking about how to use the anova() function after creating a model with lm(). You’ll do the same thing with the tetris data here.\nFirst, use factor() to do the same thing we did in the tutorial, renaming the levels of Condition from 1,2,3,4 to “Control”, etc.\n\n\ntetris$Condition <- factor(tetris$Condition, \n                           levels = 1:4,\n                           labels = c(\"Control\", \"Reactivation+Tetris\", \n                                      \"Tetris\", \"Reactivation\"))\n\nThen, run two ANOVAs: one asking whether Condition determines the number of intrusive thoughts in the days after the intervention (Days_One_to_Seven_Number_of_Intrusions), and the second asking whether Condition impacts the responses on the task that was designed to measure intrusive thoughts (Number_of_Provocation_Task_Intrusions).\nFor each ANOVA, you should also plan to report the results as we did in the tutorial—complete with means and the F(df1,df2)=value,p<.05 or p=value.\n\n\nintrusions <- lm(Days_One_to_Seven_Number_of_Intrusions ~ Condition,\n                 data = tetris)\nanova(intrusions)\n\nAnalysis of Variance Table\n\nResponse: Days_One_to_Seven_Number_of_Intrusions\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nCondition  3 114.82  38.273  3.7948 0.01409 *\nResiduals 68 685.83  10.086                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntetris %>%\n  group_by(Condition) %>%\n  summarize(mean_intrusions = mean(Days_One_to_Seven_Number_of_Intrusions))\n\n# A tibble: 4 x 2\n  Condition           mean_intrusions\n  <fct>                         <dbl>\n1 Control                        5.11\n2 Reactivation+Tetris            1.89\n3 Tetris                         3.89\n4 Reactivation                   4.83\n\nThere was a significant effect of condition predicting the number of intrusions in the seven days following the intervention, \\(F(3,68)=3.79,p<.05,\\eta^2=0.14\\), a large effect. While the number of intrusions had significantly decreased in the Reactivation+Tetris condition (\\(M=1.89\\)), they had not decreased to the same degree in the control condition (\\(M=5.11\\)), Tetris-only condition (\\(M=3.89\\)), or reactivation-only condition (\\(M=4.83\\)).\n\n\nprov.task <- lm(Number_of_Provocation_Task_Intrusions ~ Condition,\n                 data = tetris)\nanova(prov.task)\n\nAnalysis of Variance Table\n\nResponse: Number_of_Provocation_Task_Intrusions\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nCondition  3  91.44 30.4815  5.5669 0.001782 **\nResiduals 68 372.33  5.4755                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntetris %>%\n  group_by(Condition) %>%\n  summarize(mean_intrusions = mean(Number_of_Provocation_Task_Intrusions))\n\n# A tibble: 4 x 2\n  Condition           mean_intrusions\n  <fct>                         <dbl>\n1 Control                        3.39\n2 Reactivation+Tetris            1.11\n3 Tetris                         3.11\n4 Reactivation                   4.17\n\nThere was a significant effect of condition predicting the number of intrusions during the provocation task, \\(F(3,68)=5.57,p<.05,\\eta^2=0.20\\), a large effect. The Reactivation+Tetris condition has very few report intrusions (\\(M=1.11\\)), but they were higher in the other conditions, including control (\\(M=3.39\\)), Tetris-only (\\(M=3.11\\)), and reactivation-only (\\(M=4.17\\)).\nWhen you find a significant result, you should also calculate the effect size, eta-squared. Do that either using library(lsr) and the etaSquared() function, or by hand. Your choice. Then edit the above paragraphs to include the effect size.\n\n\nlibrary(lsr)\netaSquared(intrusions)\n\n             eta.sq eta.sq.part\nCondition 0.1434073   0.1434073\n\netaSquared(prov.task)\n\n            eta.sq eta.sq.part\nCondition 0.197173    0.197173\n\nI told you in class that you shouldn’t repeatedly use t-tests because of the fact that doing so increases your Type I error rate (i.e., risk of false positives). However, you can do what are called planned comparisons. After a significant result on an ANOVA, you may compare the condition of interest against the others if you intended to do so. (This is one more reason why preregistration is so important!) Usually, even when you’ve planned to do comparisons, you should do a correction for the planned comparison. In this case, we want to compare the Reactivation + Tetris condition to the others.\nR has a built-in function called pairwise.t.test() which takes three arguments: x (the dependent variable), g (the grouping variable), and p.adj (the kind of correction—here, “bonf” for Bonferroni). I’m giving you an example here for how we’d have done this with the day 0 data.\n\n\npairwise.t.test(x = tetris$Number_of_Provocation_Task_Intrusions, \n                g = tetris$Condition,\n                p.adj = \"bonf\")\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  tetris$Number_of_Provocation_Task_Intrusions and tetris$Condition \n\n                    Control Reactivation+Tetris Tetris\nReactivation+Tetris 0.0284  -                   -     \nTetris              1.0000  0.0753              -     \nReactivation        1.0000  0.0013              1.0000\n\nP value adjustment method: bonferroni \n\nThe numbers it lists are the p-values for independent-samples t-tests between separate groups—with a Bonferroni-Holms correction. You’ll see that it rates them all as 1—that’s a 100% likelihood that they came from the null hypothesis (there are no differences). Try running the pairwise.t.test() function on either of the two tests you ran in #2.\n\n\npairwise.t.test(x = tetris$Days_One_to_Seven_Number_of_Intrusions, \n                g = tetris$Condition,\n                p.adj = \"BH\")\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  tetris$Days_One_to_Seven_Number_of_Intrusions and tetris$Condition \n\n                    Control Reactivation+Tetris Tetris\nReactivation+Tetris 0.020   -                   -     \nTetris              0.378   0.126               -     \nReactivation        0.794   0.021               0.451 \n\nP value adjustment method: BH \n\nFor any significant results from your pairwise t-test, you should then run a t-test for independent samples by filtering to only Reactivation+Tetris and the groups it is different from. You’ll see that the p-value you received above is substantially closer to .05—that’s the correction!\n\n\ncrt <- tetris %>%\n  filter(Condition == \"Control\" | Condition == \"Reactivation+Tetris\")\n\nt.test(crt$Days_One_to_Seven_Number_of_Intrusions ~ crt$Condition)\n\n    Welch Two Sample t-test\n\ndata:  crt$Days_One_to_Seven_Number_of_Intrusions by crt$Condition\nt = 2.9893, df = 22.632, p-value = 0.006627\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.990331 5.454113\nsample estimates:\n            mean in group Control mean in group Reactivation+Tetris \n                         5.111111                          1.888889 \n\nrrt <- tetris %>%\n  filter(Condition == \"Reactivation\" | Condition == \"Reactivation+Tetris\")\nt.test(rrt$Days_One_to_Seven_Number_of_Intrusions ~ rrt$Condition)\n\n    Welch Two Sample t-test\n\ndata:  rrt$Days_One_to_Seven_Number_of_Intrusions by rrt$Condition\nt = -3.3228, df = 25.684, p-value = 0.002681\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.766996 -1.121893\nsample estimates:\nmean in group Reactivation+Tetris        mean in group Reactivation \n                         1.888889                          4.833333 \n\nReport the results from your t-test. Mention that the p-values are adjusted.\nPost-hoc t-tests with Bonferroni-Holms adjusted p-values demonstrated a significant difference between Reactivation+Tetris and control participants, \\(t(22.6)=2.99,p<.05\\) and between Reactivation+Tetris and Reactivation-only participants, \\(t(25.7)=3.32,p<.05\\), but not between Reactivation+Tetris and Tetris-only participants.\nFor the variable (Days_One_to_Seven_Number_of_Intrusions or Number_of_Provocation_Task_Intrusions) that you’ve been focused on, use group_by() and summarize() to calculated means and standard errors per Condition group, and then plot them using geom_col() and geom_errorbar(). Include labels and a theme.\n\n\ngraph <- tetris %>%\n  group_by(Condition) %>%\n  summarize(\n    mean_intrusions = mean(Days_One_to_Seven_Number_of_Intrusions),\n    n = n(),\n    sd = sd(Days_One_to_Seven_Number_of_Intrusions),\n    sem = sd / sqrt(n)\n  )\n\nggplot(graph, aes(x = Condition, y = mean_intrusions)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean_intrusions - sem, \n                    ymax = mean_intrusions + sem),\n                width = .4) +\n  theme_classic() +\n  labs(x = \"Condition\", y = \"Mean Number of Intrusions,\\nfrom Days 1 to 7\",\n       title = \"Number of intrusions per condition after intervention\")\n\n\nIf you’d like, try using the F cut-off value for our ANOVAs (provided below), and change the error bars from your plot in 7 to 95% Confidence Intervals. How do those look?\n\n\nqf(.95, df1 = 3, df2 = 68)\n\n[1] 2.739502\n\n\n\ngraph <- graph %>%\n  mutate(ci = sem * qf(.95, df1 = 3, df2 = 68))\nggplot(graph, aes(x = Condition, y = mean_intrusions)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean_intrusions - ci, \n                    ymax = mean_intrusions + ci),\n                width = .4) +\n  theme_classic() +\n  labs(x = \"Condition\", y = \"Mean Number of Intrusions,\\nfrom Days 1 to 7\",\n       title = \"Number of intrusions per condition after intervention\")\n\n\nHave any feedback about the exercises? Let me know at the exit survey and select Lab 9.\n\n\n",
    "preview": "answers/08-lab/anova-lab-9-completed_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2020-11-19T16:23:08-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/11-test-yourself-I/",
    "title": "Lab 08: The solo project",
    "description": "PSY 203: Lab 8.",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-10-19",
    "categories": [
      "practice",
      "rlanguage"
    ],
    "contents": "\nThe primary website for this course is https://faculty.bard.edu/~jdainerbest/psy-203\nThese are all of the instructions for the solo project. The list of labs is here.\nThis website contains R lab code for labs in Bard College’s Fall 2020 for Statistics for Psychology, taught by Prof. Justin Dainer-Best.\nYou will download the lab’s files to your computer or to your rstudio.cloud account.\nObjectives\nThis lab asks you to practice the skills you have learned about running t-tests and visualizing the results, as well as reporting them. It also expects you to practice using filter() and otherwise subsetting data. As described in the syllabus, the solo project is designed to be completed alone, and allows you to apply the code we’ve been learning in lab to a research question. This project helps you practice adapting code from previous work to a new question.\nMake sure you’re in the right working directory\nSet your working directory if necessary. Check that you’re in the directory you expect by running getwd() in the Console. If you need more help, look at the wiki page on setting a working directory.\nThe solo project\nTo download the project and the data it relies on, run the following command. Please note: only run the usethis::usezip() line once! Running it again will result in your overwriting the files on your computer—potentially losing your work.\nMake sure you run the whole command, all the way to the closing parenthesis.\n\nusethis::use_zip(paste0(\"https://github.com/jdbest/\",\n                        \"psy-203/raw/master/lab08.zip\"), \n                        cleanup = TRUE)\nFor most users, a window will pop up with the solo project. Double click on it to open it, or select it from the Files pane in RStudio.\n\n\n",
    "preview": {},
    "last_modified": "2020-11-19T16:21:58-05:00",
    "input_file": {}
  },
  {
    "path": "answers/07-lab/",
    "title": "t-tests (Lab 07) Exercises, Completed",
    "description": "Completed exercises for the seventh lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-10-18",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "t-tests"
    ],
    "contents": "\n\n\n\nThis document is meant to be used to practice after you have completed the tutorial for today’s lab. Make sure to put your name as the author of the document, above!\nIf you intend to work on these exercises while referring to the tutorial, there are instructions on the wiki on how to do so. You may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nImportant reminder: as with every time you open RStudio, don’t forget to load the libraries, below.\nObjectives\nIn the tutorial, you learned about the function pivot_longer(), which comes from the {tidyr} package (for “tidying” data). You also got to learn about running t-tests with the t.test() function, and practice using {ggplot2} functions to plot them. The exercise will let you work this out.\nTurning it in\nThis week, you must turn the completed version of this document in on Brightspace. I will provide feedback. As always, a version of these exercises with my answers will be posted at the end of the week to the lab website: https://faculty.bard.edu/~jdainerbest/psy-203/labslist.html (likely on Monday morning, 10/19).\nI encourage you to do two things as you work through this document: (1) Save it frequently! Hit the disk image above to save, hit Command/Ctrl and s, or go to the File menu and click save. When the document is saved, the title of it will go from red to black. (2) Practice Knitting the document by hitting the Knit button. You can do it now—it’ll create an HTML file in the folder where this file lives. Knitting will both help to identify problems and provide a complete document for me to review.\nLoading packages\nAs always, you must load packages if you intend to use their functions. (“Turn on the lights.”) Run the following code chunk to load necessary packages for these exercises.\n\n\nlibrary(tidyverse)\n# tidyverse loads these that we'll use today:\n# library(readr)\n# library(dplyr)\n# library(ggplot2)\n# library(tidyr)\n\nImporting data\nYou downloaded the fisher.csv data when you downloaded this file; it’s in a folder called /data. Import it below; you probably want to use the read_csv() function from the {readr} package, which was loaded in the {tidyverse}. I’d name the imported file fisher like it was in the tutorial.\nYou also re-downloaded the friends.csv data. Import that, too.\nRemember that both of these are in a folder called “/data”, so your import line might look like read_csv(\"data/friends.csv\")—make sure to have the code importing both data in the code chunk below if you want the document to knit.\n\n\nfisher <- read_csv(\"data/fisher.csv\")\nfriends <- read_csv(\"data/friends.csv\")\n\nTask\nYou have two tasks today. You should create your own code chunks (how?: instructions here) and insert all code into them.\nTask 1: following the same procedures as in the tutorial, test whether anxiety scores in the fisher dataset (hars.pre and hars.post) reduced from pre-test to post-treatment. Include a t.test() with outcomes, a plot demonstrating the results (label all axes and give it a title!), and a brief write-up describing the results including means and the results of the t-test. (You do not need to go through the steps of calculating all components—you can just use the t.test() function—but you should plan to go through the steps of hypothesis-testing.)\n\nStep 1: Define hypotheses\nNull: There is no difference between anxiety scores before and after treatment, \\(\\mu_{pre}=\\mu_{post}\\)\nResearch hypothesis: There is a difference between anxiety scores before and after treatment, \\(\\mu_{pre}\\neq\\mu_{post}\\)\nStep 2: Determine the characteristics of the comparison distribution\nBecause this is a dependent-samples t-test, the comparison distribution is the t distribution. The degrees of freedom will be equal to \\(n-1\\), the mean will be 0, and the \\(S_M\\) will be the standard deviation of the difference distribution.\nTherefore, \\(df=31\\), \\(\\mu_M=0\\), and \\(S_M\\) is the SEM.\nWe’ll integrate the last three steps into the running of the t-test:\nThe t-test:\n\n\nt.test(fisher$hars.pre, fisher$hars.post, paired=TRUE)\n\n    Paired t-test\n\ndata:  fisher$hars.pre and fisher$hars.post\nt = 8.2142, df = 31, p-value = 2.81e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  6.929815 11.507685\nsample estimates:\nmean of the differences \n                9.21875 \n\nYes, there is a significant difference—a reduction in anxiety scores, such that participants had a mean score of \\(M=15.80\\) before treatment and \\(M=6.59\\) after treatment; there is a statistically significant result to a dependent means t-test, \\(t(31)=8.21, p < .05\\).\nTwo plots to show this:\n\n\nggplot(fisher, aes(x = hars.pre, y = hars.post)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Pre-treatment scores\", y = \"Post-treatment scores\", \n       title = \"HARS scores before and after treatment\")\n\n\nWith means:\n\n\nfisher.long <- fisher %>%\n  pivot_longer(c(hars.pre, hars.post), names_to = \"time\", values_to = \"hars\")\n\n# use factor() to rename \"hars.pre\" and \"hars.post\" in the time column\nfisher.long <- fisher.long %>%\n  mutate(time = factor(time, \n                       levels = c(\"hars.pre\", \"hars.post\"),\n                       labels = c(\"Pre-Treatment\", \"Post-Treatment\")))\n\nhars.means <- fisher.long %>%\n  group_by(time) %>%\n  summarize(hars.mean = mean(hars), \n            hars.sd = sd(hars), \n            n = n(),\n            sem = hars.sd / sqrt(n),\n            .groups = \"drop_last\")\n\nggplot(hars.means, aes(x = time, y = hars.mean)) +\n  geom_col(color = \"orange\") +\n  geom_errorbar(aes(ymin = hars.mean - sem, ymax = hars.mean + sem), \n                width = .3) +\n  theme_minimal() +\n  labs(x = \"Time\", y = \"HARS score\", title = \"HARS scores before and after treatment\")\n\n\nWe could, alternatively, plot the points with geom_point() and geom_line() to connect them, with the longform data—we’ll add a group = id into the aes() to make sure the lines are per-person.\n\n\nggplot(fisher.long, aes(x = time, y = hars, group = id)) +\n  geom_point(alpha = .7) +\n  geom_line(alpha = .7) +\n  theme_minimal() +\n  labs(x = \"Time\", y = \"HARS score\", title = \"HARS scores before and after treatment\")\n\n\n\nTask 2: with the friends data, do the following:\nFilter to only people who do not have an NA for the questions operas (“How many operas have you seen in your life? (Approximately is fine.)”) and like.science (“On a scale of 1 (not at all) to 7 (very much), how much do you like science?”)\n\n\nfriends <- friends %>%\n  filter(! is.na(operas), ! is.na(like.science))\n\nUsing the ifelse() function we’ve used a few times (remember: ifelse(condition, \"if true\", \"if false\")), create a new column in friends called operaYN that simplifies this into a binary “yes I’ve seen an opera” or “no I haven’t”: operaYN = ifelse(operas > 0, \"Yes\", \"No\"). Make sure to save it to the data frame. Up to you if you use mutate()—message me or a classmate if you’re unsure on this one (feel free to use the Slack channel), but do figure it out.\n\n\nfriends <- friends %>%\n  mutate(operaYN = ifelse(operas > 0, \"Yes\", \"No\"))\n\nUsing a t-test, determine whether people who have seen an opera like science more or less than those who have not. Write up your results in the white space below the code chunk.\n\n\nt.test(friends$like.science ~ friends$operaYN)\n\n    Welch Two Sample t-test\n\ndata:  friends$like.science by friends$operaYN\nt = -1.3762, df = 67.024, p-value = 0.1733\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8501146  0.1562370\nsample estimates:\n mean in group No mean in group Yes \n         2.000000          2.346939 \n\nThere is no statistically-significant difference between groups. Individuals who have not seen operas (\\(M=2\\)) don’t like science more or less than those have seen operas (\\(M=2.34\\)), \\(t(67.0)=-1.38, p=.17\\).\nCreate a plot of the results. Don’t forget to label axes.\n\n\nggplot(friends, aes(x = operaYN, y = like.science)) +\n  geom_jitter(height = 0) +\n  theme_minimal() +\n  labs(x = \"Have you seen any operas?\", y = \"How much do you like science? (Lower = more)\",\n       title = \"Relationship between number of operas seen and enjoyment of science\")\n\n\nOr, with means:\n\n\nscience.means <- friends %>%\n  group_by(operaYN) %>%\n  summarize(science.mean = mean(like.science), \n            science.sd = sd(like.science), \n            n = n(),\n            sem = science.sd / sqrt(n),\n            .groups = \"drop_last\")\n\nggplot(science.means, aes(x = operaYN, y = science.mean)) +\n  geom_col(color = \"orange\") +\n  geom_errorbar(aes(ymin = science.mean - sem, ymax = science.mean + sem), \n                width = .3) +\n  theme_minimal() +\n  labs(x = \"Have you seen any operas?\", y = \"How much do you like science? (Lower = more)\", title = \"Relationship between number of operas seen and enjoyment of science\")\n\n\n\n\n",
    "preview": "answers/07-lab/lab-7-completed_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-11-19T16:21:12-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/06-lab/",
    "title": "Visualizing Data II (Lab 06) Exercises, Completed",
    "description": "Completed exercises for the sixth lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "group_by"
    ],
    "contents": "\n\n\n\nThis document is meant to be used to practice after you have completed the tutorial for today’s lab. Make sure to put your name as the author of the document, above!\nIf you intend to work on these exercises while referring to the tutorial, there are instructions on the wiki on how to do so. You may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nObjectives\nIn the tutorial, you practiced with removing NAs, identifying errors in code, and some more advanced plots. In today’s exercises, you’ll reflect on that.\nTurning it in?\nI invite you to turn in this document on Brightspace, where I will provide some basic feedback. You do not need to do this, but you may find it useful. As always, a version of these exercises with my answers will be posted at the end of the week to the lab website: https://faculty.bard.edu/~jdainerbest/psy-203/labslist.html\nI encourage you to do two things as you work through this document: (1) Save it frequently! Hit the disk image above to save, hit Command/Ctrl and s, or go to the File menu and click save. When the document is saved, the title of it will go from red to black. (2) Practice Knitting the document by hitting the Knit button. You can do it now—it’ll create an HTML file in the folder where this file lives. Knitting will both help to identify problems and provide a complete document for me to review.\nLoading packages\nAs always, you must load packages if you intend to use their functions. (“Turn on the lights.”) Run the following code chunk to load necessary packages for these exercises.\n\n\nlibrary(tidyverse)\n# tidyverse loads these:\n# library(dplyr)\n# library(ggplot2)\n\nImporting data\nImport the newfriends dataset in the folder lab06/data/. You can do this by going to that and clicking on it, or by running code like: load(\"data/newfriends.Rdata\"). This is a new kind of data: .Rdata files, which have R variables saved in them.\n\n\nload(\"data/newfriends.Rdata\")\n\nQuestions\nWhich errors in R are most common for you? How can you address them?\nRefer to the common mistakes and errors list\nTake the newfriends dataset. It is the same as the one we used during the tutorial. Do the following:\nUsing group_by() and summarize(), find the average number of siblings for people who do (or do not) eatmeat. Also find the sd(). Create a column for n = n(), too, and then calculate the standard error. (All within the same summarize() call.) Just have it print to the screen to make sure it’s right. Once it is, assign it to a new named variable so it’s saved to the Global Environment. Remove NAs where necessary using one of the methods discussed in the tutorial. (Make sure to create a code chunk below this for this code. Remember: you can do this from the Insert menu in this panel [click on R], or by pressing Control (or Command), Option, and i. Be sure there is a blank line above and below the code chunk.)\nNote: when using group_by() %>% summarize() in RStudio, you will likely get a warning that “summarise() ungrouping output (override with .groups argument)”. That is fine! It’s not an error; it’s just R telling you what it’s doing. If you really want to, you can add , .groups = \"drop_last\" and it will stop giving you the error.\n\n\nmeateaters <- newfriends %>%\n  filter(! is.na(eatmeat)) %>%\n  group_by(eatmeat) %>%\n  summarize(mean = mean(siblings),\n            sd   = sd(siblings), \n            n    = n(), \n            sem  = sd/sqrt(n))\n\nCreate a ggplot() plotting the results from the summary you just made (I’d put eatmeat on the x-axis and your means on the y-axis.). Add on geom_col() and geom_errorbar(), as well as a theme and labs(). Give it a meaningful title that explains your findings. Do all of this in a code chunk that you create below this.\n\n\nggplot(meateaters, aes(x = eatmeat, y = mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), \n                width = .3, size = 1.5) +\n  theme_minimal() +\n  labs(x = \"Participants' meat-eating habits\", y = \"Mean number of siblins\",\n       title = \"Relationship between number of siblings\\nand whether participants eat meat\")\n\n\nCopy the code from 2b into a code chunk you make below, and then make the fill vary based on the n, by adding something like fill = n into the aes(). (“Something like” because you may or may not have called the variable n exactly.) Which aes(), you might ask? You can do it in an aes() call in the geom_col() parentheses, or in the original ggplot() one.\n\n\nggplot(meateaters, aes(x = eatmeat, y = mean, fill = n)) +\n  geom_col(color = \"orange\", size = 2) +\n  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), \n                width = .3, size = 1.5) +\n  theme_minimal() +\n  labs(x = \"Participants' meat-eating habits\", y = \"Mean number of siblins\",\n       title = \"Relationship between number of siblings\\nand whether participants eat meat\")\n\n\nYou can just edit the code chunk above, but: add a color = \"orange\" into the geom_col() parentheses. Because it’s not based on a variable, it does not need to be inside aes() – and in fact, it should not be.\nWhat is the point of setting fill = n above? What does it gain us?\n\nIt shows us that different groups have different numbers of people in them—i.e., that we have many meat-eaters who responded but only a few non-meat-eaters or rare-meat-eaters.\n\nWhat are the error bars showing?\n\nHow much variability we anticipate in the mean. This is an idea akin to 95% confidence intervals—a larger error bar demonstrates more variability in the sample and therefore less confidence in the mean being the true mean.\n\nFilter your data to only people who plan to vote for “Biden/Harris”. Then create a ggplot() with gender on the y-axis and number of classes (numclasses) on the x-axis, and add a geom_jitter(). Also add a theme, labs(), and add color to the points.\n\n\nbidenvoters <- newfriends %>%\n  filter(votefor == \"Biden/Harris\")\nggplot(bidenvoters, aes(y = gender, x = numclasses)) +\n  geom_jitter(color = \"pink\") +\n  theme_minimal() +\n  labs(y = \"Gender\", x = \"Number of classes taken\")\n\n\nThen, find the means of the data from (5): group_by(gender) and use summarize() to get the mean, SD, n, and SEM for numclasses. Assign this to a new variable.\n\n\nbidenvoters.meanclasses <- bidenvoters %>%\n  group_by(gender) %>%\n  summarize(\n    mean = mean(numclasses), \n    sd   = sd(numclasses), \n    n    = n(),\n    sem  = sd / sqrt(n)\n  )\n\nCopy the plot from question 5. Add a layer of a geom_point() and set data = to the new data variable from question 6. Add a mapping aes() where x is the mean number of classes, y is the gender, and size = n.\n\n\nggplot(bidenvoters, aes(y = gender, x = numclasses)) +\n  geom_point(data = bidenvoters.meanclasses, aes(y = gender, x = mean, size = n)) +\n  geom_jitter(color = \"pink\") +\n  theme_minimal() +\n  labs(y = \"Gender\", x = \"Number of classes taken\")\n\n\nThat’s it!\n\n\n",
    "preview": "answers/06-lab/lab-6-completed_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-11-19T16:20:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/05-lab/",
    "title": "Refreshing and Reviewing (Lab 05) Exercises, Completed",
    "description": "Completed exercises for the fifth lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-10-02",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "dplyr",
      "t-tests"
    ],
    "contents": "\n\n\n\nThere is no tutorial for today’s lab; this document is the primary focus. Don’t forget to put your name between the quotation marks following the word author: above.\nYou may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nObjectives\nToday, you’ll practice a number of the coding features from previous labs, while also practicing some of the initial concepts of the t-test. You’ll get to do a bit of filter()ing and (gg)plotting, and then see the difference between a z-test with a sample mean (and known population mean and variance) and a one-sample t-test with a sample mean (and known population mean, but unknown population variance).\nYou can find a completed version of these exercises at https://jdbest.github.io/psychRstats/answers.html\nAs you work through this document:\nSave it frequently! Hit the disk image above to save, hit Command/Ctrl and s, or go to the File menu and click save. When the document is saved, the title of it will go from red to black.\nPractice Knitting the document by hitting the Knit button (or do it via the keyboard shortcut: Command [or Control], Shift, and K). You can do it right away—knitting will create an HTML file in the folder where this file lives. Knitting will both help to identify problems and provide a complete document.\nLoading packages\nRun the following code chunk to load necessary packages for these exercises. If you haven’t yet loaded the tidyverse documents for your file, you should do so now.\n\n\nlibrary(tidyverse)\n# tidyverse loads these:\n# library(dplyr)\n# library(ggplot2)\n\n\n\nThe data\nToday you’ll be looking at the friends dataset again.\nYou downloaded this data a few weeks ago (in lab 03). Take some time and figure out how to load it below. This is something you should know how to do, and that we’ve done in the past few weeks; I’m intentionally leaving you to it rather than giving you the data again. Assign the friends data to a variable called friends <- using the assignment operator, <-. You’ll want to do one of the following to import the data. (These are not steps.)\nFind the friends.csv file on your computer, and copy it into the same folder as this document, then run the command in the code chunk below after uncommenting it (removing the #).\nNavigate to the file in the Files pane of RStudio, click on it and select “Import”, and then import it. Then copy the code that RStudio provides in the Console into the code chunk below for when you knit this document.\nUse the File menu. Go to File: Import Dataset: From text (readr) and then find the file on your computer. Then copy the code that RStudio provides in the Console into the code chunk below for when you knit this document.\nHand-write the code: use read_csv() and write the relative (../etc.) or explicit path to the file. Don’t forget to use quotation marks for the file. You can copy the file path. Windows: Click “Copy Path” in the Home tab ribbon in file explorer. Mac: Right-click on the file, hold down the option key on your keyboard, and click ‘Copy “friends.csv” as Path name’.\nNote: if the code from (2) or (3) works, but it does not work from the code chunk, try adding two dots and a forward slash “../” at the very beginning of the quoted bit, e.g., read_csv(\"../03-exercise/friends.csv\")\nNote 2: if you get an error when you try to use read_csv(), it’s either that your path is wrong or that you didn’t load the packages, above. The former error would be, e.g., “could not find function read_csv”—go load the packages!\n\n\n# uncomment the line below if the file friends.csv is in the same folder as this document, \n# or edit it so that it correctly points to the right path for the friends.csv file\nfriends <- read_csv(\"../03-exercise/friends.csv\")\n\n\n\nWhat are the names() of all of our data frame’s columns? (There are 32 of them!)\n\n\nnames(friends)\n\n\n [1] \"duration\"            \"finished\"            \"whentook\"           \n [4] \"DistributionChannel\" \"socialmedia\"         \"gender\"             \n [7] \"siblings\"            \"smed.hrs\"            \"gram.followers\"     \n[10] \"fbfriends\"           \"tvhours\"             \"haircolor\"          \n[13] \"belief.in.god\"       \"liveoncampus\"        \"numclasses\"         \n[16] \"numclassesremote\"    \"eatmeat\"             \"operas\"             \n[19] \"cigarettes\"          \"like.science\"        \"harrypotter\"        \n[22] \"registeredtovote\"    \"votefor\"             \"expectedoutcome_1\"  \n[25] \"expectedoutcome_2\"   \"expectedoutcome_3\"   \"expectedoutcome_4\"  \n[28] \"majordiv\"            \"hrs.sleep\"           \"height.unvalidated\" \n[31] \"shootingdrills\"      \"handedness\"         \n\nHow do you select the third column of the data frame?\n\n\n# all of these work to do this:\nfriends %>% select(3)\n\n\n# A tibble: 84 x 1\n   whentook           \n   <dttm>             \n 1 2020-08-27 13:05:27\n 2 2020-08-27 14:58:31\n 3 2020-08-27 15:06:45\n 4 2020-08-27 15:26:38\n 5 2020-08-28 09:22:43\n 6 2020-08-28 11:24:18\n 7 2020-08-28 13:38:29\n 8 2020-08-28 22:13:56\n 9 2020-08-28 22:15:18\n10 2020-08-28 22:17:37\n# … with 74 more rows\n\nfriends[,3]\n\n\n# A tibble: 84 x 1\n   whentook           \n   <dttm>             \n 1 2020-08-27 13:05:27\n 2 2020-08-27 14:58:31\n 3 2020-08-27 15:06:45\n 4 2020-08-27 15:26:38\n 5 2020-08-28 09:22:43\n 6 2020-08-28 11:24:18\n 7 2020-08-28 13:38:29\n 8 2020-08-28 22:13:56\n 9 2020-08-28 22:15:18\n10 2020-08-28 22:17:37\n# … with 74 more rows\n\nfriends$whentook\n\n\n [1] \"2020-08-27 13:05:27 UTC\" \"2020-08-27 14:58:31 UTC\"\n [3] \"2020-08-27 15:06:45 UTC\" \"2020-08-27 15:26:38 UTC\"\n [5] \"2020-08-28 09:22:43 UTC\" \"2020-08-28 11:24:18 UTC\"\n [7] \"2020-08-28 13:38:29 UTC\" \"2020-08-28 22:13:56 UTC\"\n [9] \"2020-08-28 22:15:18 UTC\" \"2020-08-28 22:17:37 UTC\"\n[11] \"2020-08-29 11:17:51 UTC\" \"2020-08-29 20:01:56 UTC\"\n[13] \"2020-08-29 20:04:11 UTC\" \"2020-08-29 20:05:41 UTC\"\n[15] \"2020-08-30 14:32:09 UTC\" \"2020-08-30 14:57:50 UTC\"\n[17] \"2020-08-30 19:54:52 UTC\" \"2020-08-30 20:22:43 UTC\"\n[19] \"2020-08-30 20:24:22 UTC\" \"2020-08-30 20:30:28 UTC\"\n[21] \"2020-08-30 21:22:41 UTC\" \"2020-08-31 10:16:08 UTC\"\n[23] \"2020-08-31 10:21:27 UTC\" \"2020-08-31 10:27:37 UTC\"\n[25] \"2020-08-31 13:28:24 UTC\" \"2020-08-31 15:00:07 UTC\"\n[27] \"2020-08-31 15:19:18 UTC\" \"2020-08-31 15:21:41 UTC\"\n[29] \"2020-08-31 16:49:36 UTC\" \"2020-08-31 17:24:31 UTC\"\n[31] \"2020-08-31 18:09:26 UTC\" \"2020-08-31 18:23:30 UTC\"\n[33] \"2020-08-31 18:25:29 UTC\" \"2020-08-31 18:38:16 UTC\"\n[35] \"2020-08-31 18:39:04 UTC\" \"2020-08-31 18:39:05 UTC\"\n[37] \"2020-08-31 19:20:32 UTC\" \"2020-08-31 20:45:53 UTC\"\n[39] \"2020-08-31 20:46:03 UTC\" \"2020-08-31 21:31:27 UTC\"\n[41] \"2020-08-31 21:35:49 UTC\" \"2020-08-31 21:51:19 UTC\"\n[43] \"2020-08-31 21:56:39 UTC\" \"2020-08-31 22:08:12 UTC\"\n[45] \"2020-08-31 22:08:24 UTC\" \"2020-08-31 22:45:01 UTC\"\n[47] \"2020-08-31 23:00:03 UTC\" \"2020-08-31 23:23:15 UTC\"\n[49] \"2020-08-31 23:35:31 UTC\" \"2020-08-31 23:54:52 UTC\"\n[51] \"2020-09-01 00:02:30 UTC\" \"2020-09-01 02:15:56 UTC\"\n[53] \"2020-09-01 08:30:40 UTC\" \"2020-09-01 08:35:47 UTC\"\n[55] \"2020-09-01 11:17:48 UTC\" \"2020-09-01 11:27:09 UTC\"\n[57] \"2020-09-01 11:28:17 UTC\" \"2020-09-01 11:50:18 UTC\"\n[59] \"2020-09-01 11:50:52 UTC\" \"2020-09-01 12:01:28 UTC\"\n[61] \"2020-09-01 12:41:23 UTC\" \"2020-09-01 12:43:50 UTC\"\n[63] \"2020-09-01 13:12:15 UTC\" \"2020-09-01 14:33:10 UTC\"\n[65] \"2020-09-01 15:03:39 UTC\" \"2020-09-01 15:37:33 UTC\"\n[67] \"2020-09-01 18:55:16 UTC\" \"2020-09-01 21:30:04 UTC\"\n[69] \"2020-09-01 22:14:42 UTC\" \"2020-09-01 23:34:27 UTC\"\n[71] \"2020-09-02 10:44:29 UTC\" \"2020-09-02 10:44:53 UTC\"\n[73] \"2020-09-02 11:50:56 UTC\" \"2020-09-02 12:49:41 UTC\"\n[75] \"2020-09-02 13:39:25 UTC\" \"2020-09-02 14:01:39 UTC\"\n[77] \"2020-09-02 14:18:38 UTC\" \"2020-09-02 14:35:14 UTC\"\n[79] \"2020-09-02 17:14:31 UTC\" \"2020-09-02 17:20:38 UTC\"\n[81] \"2020-09-02 17:55:04 UTC\" \"2020-09-02 17:58:51 UTC\"\n[83] \"2020-09-02 21:15:58 UTC\" \"2020-09-06 20:48:51 UTC\"\n\nfriends %>% select(whentook)\n\n\n# A tibble: 84 x 1\n   whentook           \n   <dttm>             \n 1 2020-08-27 13:05:27\n 2 2020-08-27 14:58:31\n 3 2020-08-27 15:06:45\n 4 2020-08-27 15:26:38\n 5 2020-08-28 09:22:43\n 6 2020-08-28 11:24:18\n 7 2020-08-28 13:38:29\n 8 2020-08-28 22:13:56\n 9 2020-08-28 22:15:18\n10 2020-08-28 22:17:37\n# … with 74 more rows\n\nUse the filter() function to filter out people with only one sibling listed in the column called siblings (i.e., remove people who have only one sibling). (But people with zero siblings should be included.) Said another way: you want everyone with 0 siblings or more than one, but not one sibling.\nIf you’re not sure how to say “not” in R: not is an exclamation point, as it is in many other programming languages. Thus, writing != is like saying “not equivalent”. If you’re wanting to instead use “or”, that’s the pipe, | – as we’ve used last lab. e.g., try running the code 2+2 == 4 | 2+2 == 5 in your console. While only the former is TRUE, the “or” pipe | says \"well, one of these is true—and will read TRUE.\nNote: I’ve seen a few people use <> in their writing. This is totally fine as a note for yourself, but does not mean “not” in R.\n\n\nfriends %>% filter(siblings != 1)\n\n\n# A tibble: 49 x 32\n   duration finished whentook            DistributionCha… socialmedia\n      <dbl>    <dbl> <dttm>              <chr>                  <dbl>\n 1      332        1 2020-08-27 14:58:31 anonymous                 12\n 2      399        1 2020-08-27 15:06:45 anonymous                  1\n 3      139        1 2020-08-28 13:38:29 anonymous                  1\n 4      271        0 2020-08-28 22:15:18 anonymous                 12\n 5      290        1 2020-08-29 20:01:56 anonymous                  1\n 6      241        1 2020-08-29 20:05:41 anonymous                  1\n 7       18        0 2020-08-30 19:54:52 anonymous                 12\n 8      323        1 2020-08-30 20:22:43 anonymous                  1\n 9      405        1 2020-08-30 20:24:22 anonymous                  1\n10      261        1 2020-08-30 20:30:28 anonymous                  1\n# … with 39 more rows, and 27 more variables: gender <chr>,\n#   siblings <dbl>, smed.hrs <dbl>, gram.followers <dbl>,\n#   fbfriends <dbl>, tvhours <dbl>, haircolor <dbl>,\n#   belief.in.god <dbl>, liveoncampus <dbl>, numclasses <dbl>,\n#   numclassesremote <dbl>, eatmeat <dbl>, operas <dbl>,\n#   cigarettes <dbl>, like.science <dbl>, harrypotter <dbl>,\n#   registeredtovote <dbl>, votefor <dbl>, expectedoutcome_1 <dbl>,\n#   expectedoutcome_2 <dbl>, expectedoutcome_3 <dbl>,\n#   expectedoutcome_4 <dbl>, majordiv <dbl>, hrs.sleep <dbl>,\n#   height.unvalidated <dbl>, shootingdrills <dbl>, handedness <dbl>\n\n# or\nfriends %>% filter(siblings < 1 | siblings > 1) # remember the | means OR\n\n\n# A tibble: 49 x 32\n   duration finished whentook            DistributionCha… socialmedia\n      <dbl>    <dbl> <dttm>              <chr>                  <dbl>\n 1      332        1 2020-08-27 14:58:31 anonymous                 12\n 2      399        1 2020-08-27 15:06:45 anonymous                  1\n 3      139        1 2020-08-28 13:38:29 anonymous                  1\n 4      271        0 2020-08-28 22:15:18 anonymous                 12\n 5      290        1 2020-08-29 20:01:56 anonymous                  1\n 6      241        1 2020-08-29 20:05:41 anonymous                  1\n 7       18        0 2020-08-30 19:54:52 anonymous                 12\n 8      323        1 2020-08-30 20:22:43 anonymous                  1\n 9      405        1 2020-08-30 20:24:22 anonymous                  1\n10      261        1 2020-08-30 20:30:28 anonymous                  1\n# … with 39 more rows, and 27 more variables: gender <chr>,\n#   siblings <dbl>, smed.hrs <dbl>, gram.followers <dbl>,\n#   fbfriends <dbl>, tvhours <dbl>, haircolor <dbl>,\n#   belief.in.god <dbl>, liveoncampus <dbl>, numclasses <dbl>,\n#   numclassesremote <dbl>, eatmeat <dbl>, operas <dbl>,\n#   cigarettes <dbl>, like.science <dbl>, harrypotter <dbl>,\n#   registeredtovote <dbl>, votefor <dbl>, expectedoutcome_1 <dbl>,\n#   expectedoutcome_2 <dbl>, expectedoutcome_3 <dbl>,\n#   expectedoutcome_4 <dbl>, majordiv <dbl>, hrs.sleep <dbl>,\n#   height.unvalidated <dbl>, shootingdrills <dbl>, handedness <dbl>\n\nUse the filter() function to show only people who have an NA (meaning that they did not respond) for the sibling question. Use the is.na() function to test whether answers to that question (in column siblings) are NA. Put the column name in the parentheses *inside the filter() parentheses: filter(is.na(VARIABLENAME)). Don’t forget to “chain” it onto the friends data frame.\n\n\nfriends %>% filter(is.na(siblings))\n\n\n# A tibble: 1 x 32\n  duration finished whentook            DistributionCha… socialmedia\n     <dbl>    <dbl> <dttm>              <chr>                  <dbl>\n1       37        0 2020-08-28 09:22:43 anonymous                 NA\n# … with 27 more variables: gender <chr>, siblings <dbl>,\n#   smed.hrs <dbl>, gram.followers <dbl>, fbfriends <dbl>,\n#   tvhours <dbl>, haircolor <dbl>, belief.in.god <dbl>,\n#   liveoncampus <dbl>, numclasses <dbl>, numclassesremote <dbl>,\n#   eatmeat <dbl>, operas <dbl>, cigarettes <dbl>,\n#   like.science <dbl>, harrypotter <dbl>, registeredtovote <dbl>,\n#   votefor <dbl>, expectedoutcome_1 <dbl>, expectedoutcome_2 <dbl>,\n#   expectedoutcome_3 <dbl>, expectedoutcome_4 <dbl>, majordiv <dbl>,\n#   hrs.sleep <dbl>, height.unvalidated <dbl>, shootingdrills <dbl>,\n#   handedness <dbl>\n\nCreate a new variable called w which is assigned to 5 values that you make up, combined with c(). Then find the mean and standard deviation of the values, using that variable.\n\n\nw <- c(4, 56, 2, 3, 65)\nmean(w)\n\n\n[1] 26\n\nsd(w)\n\n\n[1] 31.66228\n\nCreate a new variable (which will become a data frame—because you’re assigning it to an already-existing data frame—you don’t have to use the data.frame() function here!). Assign it to everyone who does not have an NA for the gram.followers column. When you want to say not NA, since you’re using a function (you remember, this was two questions ago: is.na()), you put the not operator (!) in front of the function: e.g., ! is.na(VARIABLENAME).\nTrying to say this a different way: make a new variable, named whatever you like, which is assigned with the <- operator to the value of friends, and then chain on a filter() selecting only people who do NOT have NA for the gram.followers column. You will be using this NEW data-frame (without NAs in that column) for the rest of the lab.\n\n\ngrammers <- friends %>% filter( ! is.na(gram.followers))\n\n\n\nDone correctly, you should have 67 rows, as you’ll see in the Environment pane. (17 people either don’t use instagram or didn’t tell us their number of followers.)\nUse ggplot() + geom_histogram() and, also, separately hist() to create histograms of the gram.followers data from the data frame you created in the last question. Remember that in a histogram, you only define the x (not the y). In the ggplot2 version, set your binwidth to something that makes sense to you (what is an appropriate number between bins?) or set the number of bins you want to see; in the hist() version, set your breaks equal to how many bins you want to see. No worries about labeling your axes.\n\n\nggplot(grammers, aes(x = gram.followers)) +\n  geom_histogram(binwidth = 100) +\n  theme_classic()\n\n\n\nggplot(grammers, aes(x = gram.followers)) +\n  geom_histogram(bins = 10) +\n  theme_classic()\n\n\n\nhist(grammers$gram.followers, breaks = 10)\n\n\n\n\nFind all of the measures of central tendency for the gram.followers column from your data frame: mean(), median(), sd(), range (min() and max() or, if you prefer, max()-min()). Use your new data frame without NAs. You don’t need to write the answers anywhere. Just your code. When you knit the document, they’ll all print (you don’t need to save them to new variables yet).\n\n\nmean(grammers$gram.followers)\n\n\n[1] 685.6269\n\nsd(grammers$gram.followers)\n\n\n[1] 439.0476\n\nmedian(grammers$gram.followers)\n\n\n[1] 600\n\nc(min(grammers$gram.followers), max(grammers$gram.followers))\n\n\n[1]   57 2000\n\n# and there actually is a function called range which does just that:\nrange(grammers$gram.followers)\n\n\n[1]   57 2000\n\n# You can also do this with a `dplyr` function called `summarize()`---the benefit\n# being that you don't have to re-write the name of the data frame each time---\n# and that it's all in one place:\ngrammers %>%\n  summarize(mean = mean(gram.followers),\n            sd = sd(gram.followers),\n            median = median(gram.followers),\n            range = max(gram.followers) - min(gram.followers))\n\n\n# A tibble: 1 x 4\n   mean    sd median range\n  <dbl> <dbl>  <dbl> <dbl>\n1  686.  439.    600  1943\n\nUse ggplot2 to create a ggplot() which adds on a geom_point() layer to plot gram.followers on the y-axis and siblings on the x-axis. This probably won’t mean a ton, but that’s okay. Be sure to label your x and y axes with full names and give the plot a meaningful title with the labs() added on. Consider adding a theme. Also consider adding some color: can you make the points blue?\n\n\nggplot(grammers, aes(x = siblings, y = gram.followers)) +\n  geom_point(color = \"blue\") + \n  theme_bw() +\n  labs(x = \"Number of Siblings\", y = \"Number of Instagram Followers\", \n       title = \"Rel. b/w siblings and facebook friends\")\n\n\n\n\nUncomment (remove the # marks from) the code chunk immediately below, replacing “DATA” with your data frame’s name. Then run the chunk to recode the 1 or 2 answers in column haircolor to be the answers to the question “Is your current hair color your natural hair color?”\nPlease note: only run this once! If you run it a second time, it will turn the whole haircolor column to NAs. Want to check what it looks like? Just highlight the part after the assignment <- operator and run that in the console.\n\n\ngrammers$haircolor <- factor(grammers$haircolor,\n                             levels = c(1, 2),\n                             labels = c(\"Natural\", \"Dyed\"))\n\n\n\nCopy the code for the plot from Q9 in the code chunk labeled ex10 below. If you added blue points, delete the color = \"blue\" in the parentheses of geom_point(). Into the parentheses of the aes() mapping in the ggplot() call at the beginning of the code chunk below, set color = equal to the hair color variable (haircolor)—careful: neither gets quotation marks! Then, inside the geom_point() command’s parentheses, set shape = \"+\" and (with a comma after the closing quotation mark in “+”) set size = 5. You’re changing the points to + signs and making them larger.\n\n\nggplot(grammers, aes(x = siblings, y = gram.followers, color = haircolor)) +\n  geom_point(shape = \"+\", size = 5) + \n  theme_bw() +\n  labs(x = \"Number of Siblings\", y = \"Number of Instagram Followers\", \n       title = \"Relationship b/w siblings and instagram followers\")\n\n\n\n\nz-tests for a single sample\nSuppose we are interested in whether our sample has, on average, the same number of instagram followers as people in general. We could say our research hypothesis is that our sample has a different mean number of followers, but the null hypothesis is that it does. (Step 1!)\nYou might say then that:\n\\[H_0: \\mu_{our~data}=\\mu_{people~in~general}\\]\n\\[H_1: \\mu_{our~data}\\neq{}\\mu_{people~in~general}\\]\nNow, how do we define people in general? This is pretty tricky in most cases—which is why, quite soon, we’re going to stop using a z-test at all. However, we can make something up—suppose that the average number of followers is 500. (I tried to find a real number online, and failed. Think you have a better estimate? Feel free to use yours instead.) There are a lot of accounts with very few followers, and a lot with thousands (and some with millions), but imagine that it averages out to 500. Is our sample different from that figure? To figure this out, to start, let’s also imagine that the standard deviation for the population distribution is 300. Again, that’s unlikely! But we’re going to roll with it for today.\nStep 2: Determine the characteristics of the comparison distribution.\nOkay, let’s get the information about our sample and therefore define the comparison distribution. We’ll use this info for calculating z. First, let’s assign the mean of the gram.followers to a new variable called follow.mean. Remember, we’re still using the new data-frame you created in #6.\n\n\nfollow.mean <- mean(grammers$gram.followers)\n\n\n\nWe don’t need to calculate the standard deviation—we’ve got it (even though it’s fake). Assign the number I gave you for the population standard deviation (300) as the variablefollow.sd.\n\n\nfollow.sd <- 300\n\n\n\nTo get the actual standard deviation of the comparison distribution, we want the sampling distribution of the mean, which has a standard deviation equal to the standard error of the mean—that’s \\(SEM=\\frac{SD}{\\sqrt{n}}\\) and we already know SD (you just saved it in a variable!). That’s easy to calculate in R. Many of you did it the other day! Just use the sqrt() function to find square roots, and use the good old-fashioned [forward] slash / for dividing. Go ahead: assign the SEM to a variable called follow.sem.\n(Note: You can find the n by just looking at the length of the data frame—it’s actually the number of rows I told you way back in Q6. But if you’d like to use an R function for it, there’s nrow() which you can run on the data frame: nrow(DATA) will give you how many rows there are in your data frame if you replace DATA with its name—in this case, one per person.)\n\n\nfollow.sem <- follow.sd / sqrt(nrow(grammers))\n\n\n\nSo, we can define our comparison distribution as follows: it is a z-distribution based on the sampling distribution of the mean, which has a mean of 685.63 and a standard deviation equal to the standard error of the mean, 36.65.\nStep 3: Determine the sample cutoff score to reject the null hypothesis\nNo changes yet! Still using the same cut-offs from last lab, with a significance level of \\(p < .05\\). We’ll continue with qnorm() for this, which is the quantiles (i.e., you’re passing it the percentile and it’s giving you what number falls there) of the normal distribution.\n\n\nqnorm(c(.025, .975)) %>% round(digits = 2)\n\n\n[1] -1.96  1.96\n\nStep 4: Determine your sample’s score\nOkay, let’s do it. Calculate z below. Assign it to a variable called follow.z and then print it out by re-writing the name on its own line. Remember: we’re using our sample mean that you just calculated, our sample’s SEM that you calculated (as an estimate of the sampling distribution’s standard deviation), and the number 500 as our fake population mean for the comparison distribution. When you divide, don’t forget to use parentheses for the numerator (on top).\n\\[z=\\frac{\\mu_{sample}-\\mu_{population}}{SEM}\\]\n\n\nfollow.z <- (follow.mean - 500) / follow.sem\nfollow.z\n\n\n[1] 5.064738\n\nOkay, our z-score here is 5.06.\nStep 5: Decide whether or not to reject the null hypothesis\nYou can likely tell straight away that this mean is statistically different from the (fake) population mean—the z-value that you calculated should be well above the cut-off. Just to practice, uncomment the third and seventh lines in the code below (beginning with follow.z and abs(follow.z)), and run the code chunk to confirm that. It should read TRUE, twice.\nWhat are we doing here? We’re testing: does the sample mean for our sample differ from the (fake) population mean and SD that I gave you?\n\n\n# check if follow.z is more than 1.96 OR less than -1.96; \n# if it is either (it can't be both), return TRUE\nfollow.z > 1.96 | follow.z < -1.96\n\n\n[1] TRUE\n\n# alternatively, use the abs() function to check if the MAGNITUDE of z\n# is more than 1.96\nabs(follow.z) > 1.96\n\n\n[1] TRUE\n\nIt does!\nWhat percentile is it? Again, uncomment to see. Here, we’re using the pnorm() function we used last week, which is the reverse of qnorm(). Where qnorm() takes a percentile and gives a z-score, pnorm() takes a z-score and gives a percentile. (No, really—try running it on -1.96 and you’ll get just about .05, and on 1.96 where you’ll get just about .975.)\n\n\npnorm(follow.z)\n\n\n[1] 0.9999998\n\nYou should get a .999999. What does that mean? It means that this score is so high that it must not come from the same distribution. It’s saying that the score’s from the 99.99999th percentile.\n… it also probably means that the fake population mean and SD are wrong, huh? Yeah, I think that fake population SD was off, even if the mean isn’t.\nSwitching from z to t\nNow we’ll try to do this whole thing with a slight difference: looking at the most basic kind of t-score. As we will (or have) discuss[ed] in class, a t-test has one primary difference from the basic z-test: rather than assuming that we know the population’s standard deviation, we instead accept that we do not. All we know in this instance is the population mean. (We’re still using the fake population mean of 500—we’re just no longer using a fake population standard deviation!)\nWhat do we need to do to find an estimate of the population standard deviation? The following things, none of which are actually knew.\nDegrees of freedom: in R language, that’s df <- n-1; mathematically,\n\\[df = n - 1\\]\nThe Sum of Squares (also known as the SS) which in R language is SS <- sum((x-mean(x))^2);\n\\[SS = \\sum{(X-M)^2}\\]\nThe estimate of the population SD, in R, sd.est <- sd(x) or, if you’re doing it by hand, sd.est <- sqrt(SS/df) (using the two variables we’ve defined above!);\n\\[S=\\sqrt{\\frac{\\sum{(X-M)^2}}{n-1}}=\\sqrt{\\frac{SS}{df}}\\]\nThe estimate of the t-distribution’s SD, which is to say, using those variables above, that’s t.sd <- sd.est/sqrt(n);\n\\[S_M=\\frac{S}{\\sqrt{n}}\\]\nStep 1\nThis is identical to the way we wrote them before. To recap:\n\\[H_0: \\mu_{our~data}=\\mu_{people~in~general}\\]\n\\[H_1: \\mu_{our~data}\\neq{}\\mu_{people~in~general}\\]\nStep 2: Find the characteristics of the comparison distribution\nUse the information above and the code below to calculate the new SD.\nI’ll walk you through this with dummy code (code that doesn’t work because it has stand-ins for your variables); you replace the dummy code with the variable names from our data. Replace the text that says DATA with the name of your data frame—still using the one we created that doesn’t have NAs. Replace the text that says COLUMN with the name of the column of interest (gram.followers, still). You should also uncomment the lines, of course, if you want them to run—remove the # marks.\n\n\n# n <- nrow(DATA)\n# df <- n - 1\n# SS <- sum((DATA$COLUMN - mean(DATA$COLUMN))^2)\n# sd.estimate <- sqrt(SS / df)\n# s.m <- sd.estimate / sqrt(n)\n# \n# # now, run the straightforward check:\n# sd.estimate2 <- sd(DATA$COLUMN)\n# s.m2 <- sd.estimate2 / sqrt(n)\n# \n# # and are these equal?: these should both read TRUE\n# identical(s.m, s.m2)\n\n\n\n\n\nn <- nrow(grammers)\ndf <- n - 1\nSS <- sum((grammers$gram.followers - mean(grammers$gram.followers))^2)\nsd.estimate <- sqrt(SS / df)\ns.m <- sd.estimate / sqrt(n)\n\n# now, run the straightforward check:\nsd.estimate2 <- sd(grammers$gram.followers)\ns.m2 <- sd.estimate2 / sqrt(n)\n\n# and are these equal? use the identical() function to test that thing: these should both read TRUE\nidentical(sd.estimate, sd.estimate2)\n\n\n[1] TRUE\n\nidentical(s.m, s.m2)\n\n\n[1] TRUE\n\nTake time and make you understand what is happening at every step—can you imagine what’s happening if you were writing the equations out on paper instead of in R code? Any questions? Ask your classmates in your breakout room. If they don’t know, ask a course assistant or the professor.\nWe have now defined our comparison distribution!\nIt’s the t-distribution with df of 66, a mean of 500, and a standard deviation (the standard error of the mean) of 53.64.\nStep 3\nWe’ll use t-tables in class like with the z-table, but in R we can do this much more easily. Remember the qnorm() function we used above? As we discussed, the “norm” bit was referring to the normal distribution. The q piece in the name meant “quantile.” Well, we can get the quantiles of the t distribution the same way: with qt(). The only difference is that we need to specify the degrees of freedom, which we defined above in Q14.\nI’m going to write it below—you run the code. It should look almost identical to the qnorm() function above. But it will result in different cut-off values, because it’s no longer using the normal distribution. They are quite similar, though, right?\n\n\nqt(c(.025, .975), df = 66)\n\n\n[1] -1.996564  1.996564\n\nOnce you get the qt() code working, copy it below, and assign it to a new variable called cutoff. (It should have two values: the upper and lower values.)\n\n\ncutoff <- qt(c(.025, .975), df = df) # using the df variable from above---which is 66\n\n\n\nStep 4: Determine your sample’s score\nOkay, now we can calculate t. It looks quite similar to z, with the exception (again) that the only thing we’re claiming to “know” at this point is the population mean—the \\(S_M\\) is coming from our estimate of the population standard deviation:\n\\[t=\\frac{M-\\mu_M}{S_M}\\]\nCreate a new variable called t.stat which is assigned <- to the value of t—in parentheses, subtract the fake population mean (500) from your mean number of followers (which you created way above in Q11 and named follow.mean, if you did as I suggested), and then divide that result by the s.m we saved in Q14—the standard error of the mean. Then just write the name (t.stat) on its own line so it prints. Don’t forget to use parentheses for the numerator (the subtraction) so it happens first.\n\n\nt.stat <- (follow.mean - 500) / s.m\nt.stat\n\n\n[1] 3.460721\n\nOkay, so we can say that $t=$3.4607211.\nStep 5: Decide whether or not to reject the null hypothesis\nIs t.stat more extreme than the critical value?\nUse one of the methods from Q13 to compare t.stat to the cutoff variable. You should also print them both and take a look—your eye may be better at deciding than code. If you saved both the lower and upper cutoffs (i.e., one negative and one positive), then it seems likely that you may want to either compare absolute values or use the subsetting square brackets, e.g., cutoff[1] and cutoff[2] for your comparison.\n\n\nt.stat\n\n\n[1] 3.460721\n\ncutoff\n\n\n[1] -1.996564  1.996564\n\nabs(t.stat) > abs(cutoff)\n\n\n[1] TRUE TRUE\n\n# the most complete: is it lower than the low cutoff or higher than the high?\nt.stat < cutoff[1] | t.stat > cutoff[2]\n\n\n[1] TRUE\n\nShould you reject the null hypothesis for this test?\n\nYes, definitely! Our t is larger than the cut-off, so it is statistically significant and p < .05.\n\nOkay, one last thing: we can also calculate the exact p-value here using the pt() function.\nThis is essentially the same as pnorm(), which we used to find the percentile of the normal distribution. Here, we want the percentile of how rare this score is, but in the t distribution. As with the qt() function, we need to give a df (the degrees of freedom). You defined that above, so un-comment these lines and run them.\nNote: What does lower.tail mean? Well, the first one is defining the probability that the true (population) mean is more than the value of your t-score; the second is the probability that the true (population) mean is less than the value of your t-score. Then we add them together to get the whole thing. The lower tail is looking all the way at the left—the extreme negative values. And the upper tail is looking all the way at the right—the extreme positive values. We use both of them to get a p-value because we’re using a two-tailed test.\n\n\nupper.tail <- pt(t.stat, df = df, lower.tail = FALSE)\nlower.tail <- pt(-t.stat, df = df, lower.tail = TRUE)\n(p.value <- upper.tail + lower.tail)\n\n\n[1] 0.000949337\n\nSo does your conclusion from #18 make sense? Remember: the comparison we make here is between the cut-off t-value and the actual t-value for our sample. The p-value tells us in a glance whether that t is larger than the cut-off, but the comparison is about the t-value.\n\nYes, it does! The p-value is definitely smaller than .05, which makes sense, given that our t is larger than the cut-off.\n\nObviously, we don’t normally do it this way. We use a t-test function! In fact, that’s t.test()—the one function for today. For the one-sample t-test, you give the t.test() function two arguments: your x-values (here, DATA$gram.followers where DATA is your data frame), and your population mean (mu), which is here 500. Because this is not a dplyr function, you do need to explicitly name the data frame as well as the column, with the $ separating them. Uncomment the code below and replace the all caps with your variables / values.\n\n\n# t.test(x = DATA$COLUMN, mu = POPULATIONMEAN)\nt.test(x = grammers$gram.followers, mu = 500)\n\n\n\n    One Sample t-test\n\ndata:  grammers$gram.followers\nt = 3.4607, df = 66, p-value = 0.0009493\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 578.5347 792.7190\nsample estimates:\nmean of x \n 685.6269 \n\nAre the t-value, df, and p-value the same? Explain. (Also: note the confidence interval around the mean!)\n\nYes, they are identical. The confidence interval is an estimate of the true mean.\n\nOkay, that’s it! Go ahead and save, then knit the document.\n\n\n\n",
    "preview": "answers/05-lab/lab-05-completed_files/figure-html5/ex7-1.png",
    "last_modified": "2021-01-29T14:04:08-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/04-lab/",
    "title": "Hypothesis Testing (Lab 04) Exercises, Completed",
    "description": "Completed exercises for the fourth lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-09-25",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "dplyr",
      "distributions"
    ],
    "contents": "\n\n\n\nThis document is meant to be used to practice after you have completed the tutorial for today’s lab. You can find a completed version of these exercises at https://jdbest.github.io/psychRstats/answers.html\nIf you would like to refer to the tutorial while also working on this document, there are instructions on the wiki on how to do so.\nYou may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nObjectives\nIn the tutorial, you put some of the work we did in class into practice in exploring a hypothesis test. Instead of using the friends data we focused on in the tutorial, we’ll be using some real data for this example.\nI will again encourage you to do two things as you work through this document: (1) Save it frequently! Hit the disk image above to save, hit Command/Ctrl and s, or go to the File menu and click save. When the document is saved, the title of it will go from red to black. (2) Practice Knitting the document by hitting the Knit button. You can do it now—it’ll create an HTML file in the folder where this file lives. Knitting will help to identify problems and provide a complete document for sharing\nLoading packages\nRun the following code chunk to load necessary packages for these exercises.\n\n\nlibrary(tidyverse)\n# tidyverse loads these:\n# library(dplyr)\n# library(ggplot2)\n\n\n\nPracticing a hypothesis test\nWe’ll return to importing new data next week—for today, just run the following chunk to load the data.\n\n\ntvfriends <- structure(list(tvhours = c(4, 1, 0, 0, 5, 4, 0, 0, 15, 8, 2, \n4, 0, 5, 4, 3, 7, 5, 14, 0, 6, 3, 0, 8, 0, 1, 10, 14, 15, 6, \n4.5, 17, 7, 10, 7, 3, 2, 14, 5, 5, 2, 2, 2, 7, 7, 4, 7, 10, 3, \n5, 20, 14, 1, 8, 3, 7, 0.5, 8, 3, 1, 20, 2, 1, 15, 14, 5, 1, \n0, 4), gender = c(\"female\", \"female\", \"male\", \"female\", \"female\", \n\"female\", \"female\", \"non-binary\", \"female\", \"male\", \"female\", \n\"female\", \"agender\", \"male\", \"male\", \"female\", \"female\", \"female\", \n\"female\", \"female\", \"female\", \"male\", \"male\", \"female\", \"male\", \n\"male\", \"female\", \"male\", \"female\", \"male\", \"male\", \"female\", \n\"female\", \"female\", \"male\", \"male\", \"female\", \"female\", \"non-binary\", \n\"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"female\", \n\"male\", \"male\", \"female\", \"male\", \"female\", \"female\", \"female\", \n\"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \n\"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"female\", \n\"male\", \"male\")), row.names = c(NA, -69L), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"))\n\n\n\nAs you did in the tutorial, you’re going to walk through the five steps of hypothesis-testing, calculate the z-score, determine the cut-off value, and get a p-value.\nWould someone who doesn’t watch any TV be considered a statistically-significant outlier among Bard students?\nTake a look at the structure of your data using the str() function or by clicking on the data in the Environment pane.\n\n\nstr(tvfriends)\n\n\ntibble [69 × 2] (S3: tbl_df/tbl/data.frame)\n $ tvhours: num [1:69] 4 1 0 0 5 4 0 0 15 8 ...\n $ gender : chr [1:69] \"female\" \"female\" \"male\" \"female\" ...\n\nRestate question as a research and null hypothesis\nWrite the research and null hypotheses, and then frame them in terms of means.\n\nResearch hypothesis: this person has a different tv-watching habit than other Bard students\n\n\nNull hypothesis: this person is no different from other Bard students\n\nStatistical framing:\n\nResearch hypothesis: \\(\\mu_{\\mathrm{this~person's~tv~hours}}\\neq\\mu_{\\mathrm{Bard~students'~tv~hours}}\\)\n\n\nNull hypothesis: \\(\\mu_{\\mathrm{this~person's~tv~hours}}=\\mu_{\\mathrm{Bard~students'~tv~hours}}\\)\n\nBefore we continue: what’s your hypothesis? Do you think 0 hours fits with most Bard students?\nDetermine the characteristics of the comparison distribution\nDescribe the z-distribution.\n\nThe z-distribution is normally-distributed and symmetrical, and has a mean of 0 and an SD of 1.\n\n\n\n\nDetermine the sample cutoff score to reject the null hypothesis.\nFirst, what cutoff do you expect for a two-tailed distribution of z-scores?\n\nI expect to see a cutoff of \\(\\pm1.96\\)\n\nThen, find the code from the tutorial. Run it here.\n\n\nqnorm(c(.025, .975)) %>% round(digits = 2)\n\n\n[1] -1.96  1.96\n\nDetermine your sample’s score\nPlot a boxplot of the data, and a histogram.\n\n\nggplot(tvfriends, aes(x = tvhours)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\nggplot(tvfriends, aes(x = tvhours)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal()\n\n\n\n\nRemember, the question was “How many hours of Netflix/Hulu/TV do you watch per week?” Do any of the data seem unreasonable? Should you filter it? Describe the histogram.\n\nTotally reasonable values. I wouldn’t filter anything. The histogram is not normal; there’s an apparent floor effect (a bunching up of values around the bottom end).\n\nCalculate the mean and standard deviation, and save them. Calculate your z for someone who watches 0 hours of TV a week. (x = 0)\n\n\nmeantv <- mean(tvfriends$tvhours)\nsdtv <- sd(tvfriends$tvhours)\nz <- (0 - meantv) / sdtv\nz\n\n\n[1] -1.103732\n\nDecide whether or not to reject the null hypothesis\nBased on the cut-off defined in Step 3, should we reject the null?\n\nNo, this is a perfectly reasonable value of -1.10. You cannot reject the null.\n\nWhat conclusion should you draw based on that?\n\nWatching no TV each week is perfectly normal behavior for Bard students. We shouldn’t assume that doing so is statistically significant.\n\nTry just running the pnorm() function on the z-value. Then, below the code chunk, explain what percentile this person is in.\n\n\npnorm(z)\n\n\n[1] 0.1348546\n\n\nThis person is in the 13th percentile—low, but not by much.\n\nTheir actual p-value:\n\n\npnorm(z, lower.tail = TRUE) + pnorm(-z, lower.tail = FALSE)\n\n\n[1] 0.2697093\n\nSo, for this person, \\(p=\\) 0.27.\nPart II: Work through the steps on your own for a one-sided test\nWould that score (0 tvhours) be statistically significant if we’re only interested in students who watch significantly less TV than other Bard students? Just write the responses below this line, and use code as necessary.\nWrite the research and null hypotheses, and then frame them in terms of means.\n\nResearch hypothesis: this person watches less TV than other Bard students\n\n\nNull hypothesis: this person watches more or the same amount of TV as other Bard students\n\nStatistical framing:\n\nResearch hypothesis: \\(\\mu_{\\mathrm{this~person's~tv~hours}}<\\mu_{\\mathrm{Bard~students'~tv~hours}}\\)\n\n\nNull hypothesis: \\(\\mu_{\\mathrm{this~person's~tv~hours}}\\ge{}\\mu_{\\mathrm{Bard~students'~tv~hours}}\\)\n\nDetermine the characteristics of the comparison distribution\n\nThe distribution is the same as above.\n\nDetermine the sample cutoff score to reject the null hypothesis.\n\nI expect to see a cutoff of -1.64\n\n\n\nqnorm(.05) %>% round(digits = 2)\n\n\n[1] -1.64\n\nDetermine your sample’s score\n\nWe’ve already done this, and it does not change.\n\n\n\nz\n\n\n[1] -1.103732\n\nDecide whether or not to reject the null hypothesis\n\nBased on the cut-off defined in Step 3, we still don’t reject the null. Watching no TV each week is still normal behavior for Bard students. We shouldn’t assume that doing so is statistically significant.\n\n\n\npnorm(z, lower.tail = TRUE)\n\n\n[1] 0.1348546\n\nFor this person’s score, \\(p=.13\\).\nExtension exercise\nIf you have more time, want to practice, or are just interested, this is another exercise of the same vein.\nThis is a summary of data from Pro Publica, an investigative journalism organization. You can read more about the data here: https://www.propublica.org/article/so-sue-them-what-weve-learned-about-the-debt-collection-lawsuit-machine\nType\nMean\nSD\nAuto\n109.52\n236.87\nCollection Agency\n39.26\n93.39\nDebt Buyer\n316.98\n867.30\nGovernment\n121.04\n190.46\nHigh-Cost Lender\n46.00\n53.49\nInsurance\n128.13\n218.79\nMajor Bank\n690.02\n1440.31\nMedical\n0.51\n8.91\nMisc\n36.92\n56.82\nMisc Lender\n69.80\n158.43\nOther\n23319.00\n5151.45\nUtility\n53.14\n55.50\nEssentially, these data show how often (over 13 years from 2001-2014) the owners of individuals’ debt in Miami-Dade County, Fl sued those individuals. It’s sorted by type of debt.\nYour task: In 2000, debt buyers (firms that buy debt to collect) sued individuals 59.8 times. This is your x. Follow the steps of hypothesis-testing to determine whether this is significantly different from the norm over the subsequent 13 years, using a z-test and the above information. This time, use a cut-off of \\(p=.01\\), so there is only a significant difference if \\(p<.01\\).\nWrite the research and null hypotheses, and then frame them in terms of means.\n\nResearch hypothesis: in 2000, debt buyers sued debtors either more or less than they did in the 13 years after Null hypothesis: in 2000, debt buyers did not sue debtors differently\n\nStatistical framing:\n\nResearch hypothesis: \\(\\mu_{\\mathrm{debt~buyers~2000}}\\neq\\mu_{\\mathrm{debt~buyers~2001-2014}}\\) Null hypothesis: \\(\\mu_{\\mathrm{debt~buyers~2000}}={}\\mu_{\\mathrm{debt~buyers~2001-2014}}\\)\n\nDetermine the characteristics of the comparison distribution\n\nThe distribution is the same as above; we’re still using a z-distribution\n\nDetermine the sample cutoff score to reject the null hypothesis.\n\nI expect to see a cutoff of \\(\\pm2.57\\) because this is a two-tailed test with \\(p<.01\\)\n\n\n\nqnorm(c(.005, .995))\n\n\n[1] -2.575829  2.575829\n\nDetermine your sample’s score\n\n\nz <- (59.8 - 316.98) / 867.3\nz\n\n\n[1] -0.2965295\n\nDecide whether or not to reject the null hypothesis\n\nBased on the cut-off defined in Step 3, we definitely don’t reject the null. While there is a clear increase in the mean, the standard deviation is so large that it doesn’t significantly differ according to this test—it’s lower, but not hugely so.\n\n\n\npnorm(z, lower.tail = TRUE) + pnorm(-z, lower.tail = FALSE)\n\n\n[1] 0.7668258\n\nFor 2000, the score has a p-value of \\(p=\\) 0.77\n\n\n\n",
    "preview": "answers/04-lab/lab-4-completed_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-01-29T12:19:56-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/03-lab/",
    "title": "Visualizing Data (Lab 03) Exercises, Completed",
    "description": "Completed exercises for the third lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-09-19",
    "categories": [
      "rlanguage",
      "visualization",
      "ggplot",
      "dplyr"
    ],
    "contents": "\n\n\n\nThis is an R Markdown document. When you execute code within the notebook, the results appear beneath the code. You can find a completed version of these exercises at https://jdbest.github.io/psychRstats/answers.html\nFeel free to change the top to have your name! You’re the person answering these exercises, now.\nIf you would like to refer to the tutorial while also working on this document, there are instructions on the wiki on how to do so.\nYou may also want to refer to past labs. Don’t forget that previous labs are linked to on the main labs website.\nObjectives\nIn the tutorial, you learned how to manipulate and visualize data in a few ways. In today’s exercise, you’ll practice doing those with the friends data that students in PSY 203 collected by asking friends to provide information. (This data is somewhat edited by the instructor and contains no identifying information.)\nYou’ll also get to continue learning about the filter() function and other ways of manipulating data.\nI encourage you to do two things as you work through this document: (1) Save it frequently! Hit the disk image above to save, hit Command/Ctrl and s, or go to the File menu and click save. When the document is saved, the title of it will go from red to black. (2) Practice Knitting the document by hitting the Knit button. You can do it now—it’ll create an HTML file in the folder where this file lives. Knitting will both help to identify problems and provide a complete document for me to review.\nLoading packages\nRun the following code chunk to load necessary packages for these exercises. If any don’t load, install them with the install.packages(\"\") function.\n\n\nlibrary(tidyverse)\n# tidyverse loads these:\n# library(dplyr)\n# library(ggplot2)\n\n\n\nImporting data\nWhen you downloaded this document, you also downloaded a file called friends.csv. In the empty code chunk below, import those data now, using the read_csv() function and assigning it to a variable which I recommend you call friends. Don’t remember how to import data? We talked about it in Lab 2 (and you can follow that link to the section where we did so).\n(If the code below doesn’t work on its own, try adding 03-lab/ before the name \"friends.csv\", and again in the code chunk below. Because of where it looks for the file when knitting vs. running interactively, you may well find that it needs no directory to knit, but needs the specific directory included to run right now.)\n\n\nfriends <- read_csv(\"friends.csv\")\n\n\n\nRemember that red text reading “Parsed with column specification:” and some info is just R explaining how it “read” the file—not an error!\nFinding out the structure\nUse the space below to find out the following:\nWhat is the str() of the data?\nWhat are the names() of the columns?\nCan you print the head() of the data frame?\nPick a column from friends. What is the class() of that column?\n\n\nstr(friends)\n\n\ntibble [84 × 32] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ duration           : num [1:84] 464 332 399 328 37 ...\n $ finished           : num [1:84] 1 1 1 1 0 1 1 0 0 1 ...\n $ whentook           : POSIXct[1:84], format: \"2020-08-27 13:05:27\" ...\n $ DistributionChannel: chr [1:84] \"anonymous\" \"anonymous\" \"anonymous\" \"anonymous\" ...\n $ socialmedia        : num [1:84] 1 12 1 12 NA 1 1 12 12 1 ...\n $ gender             : chr [1:84] \"Female\" \"female\" \"Male\" \"Female\" ...\n $ siblings           : num [1:84] 1 3 5 1 NA 1 3 1 2 1 ...\n $ smed.hrs           : num [1:84] 1 5 2 2 NA 1 3 1 4 1.5 ...\n $ gram.followers     : num [1:84] 158 800 337 400 NA 250 640 NA NA 439 ...\n $ fbfriends          : num [1:84] NA 1000 NA 500 NA NA NA NA NA NA ...\n $ tvhours            : num [1:84] 4 1 0 0 NA 5 4 NA NA 10 ...\n $ haircolor          : num [1:84] 2 1 1 1 NA 2 1 NA NA 2 ...\n $ belief.in.god      : num [1:84] 3 1 2 1 NA 2 2 NA NA 3 ...\n $ liveoncampus       : num [1:84] 1 1 1 2 NA 1 1 NA NA 1 ...\n $ numclasses         : num [1:84] 5 5 4 4 NA 5 4 NA NA 4 ...\n $ numclassesremote   : num [1:84] 0 2 0 4 NA 0 1 NA NA 1 ...\n $ eatmeat            : num [1:84] 1 2 3 2 NA 2 1 NA NA 3 ...\n $ operas             : num [1:84] 4 1 0 1 NA 2 0 NA NA 1 ...\n $ cigarettes         : num [1:84] 2 2 2 2 NA 2 2 NA NA 2 ...\n $ like.science       : num [1:84] 2 2 1 2 NA 1 2 NA NA 3 ...\n $ harrypotter        : num [1:84] 7 0 0 0 NA 7 7 NA NA 4 ...\n $ registeredtovote   : num [1:84] 1 4 1 1 NA 3 1 NA NA 1 ...\n $ votefor            : num [1:84] 1 5 1 2 NA 1 1 NA NA 1 ...\n $ expectedoutcome_1  : num [1:84] 70 98 41 65 NA 30 70 NA NA 50 ...\n $ expectedoutcome_2  : num [1:84] NA 30 25 38 NA 30 70 NA NA 40 ...\n $ expectedoutcome_3  : num [1:84] NA 80 45 53 NA 30 70 NA NA 30 ...\n $ expectedoutcome_4  : num [1:84] 100 0 100 85 NA 90 100 NA NA 100 ...\n $ majordiv           : num [1:84] 3 4 4 2 NA 4 2 NA NA 3 ...\n $ hrs.sleep          : num [1:84] 9 8 8 7 NA 8 7 NA NA 8 ...\n $ height.unvalidated : num [1:84] 60 60 74 62 NA 65 65 NA NA 67 ...\n $ shootingdrills     : num [1:84] 1 1 1 2 NA 1 1 NA NA 1 ...\n $ handedness         : num [1:84] 4 4 4 3 NA 4 4 NA NA 4 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   duration = col_double(),\n  ..   finished = col_double(),\n  ..   whentook = col_datetime(format = \"\"),\n  ..   DistributionChannel = col_character(),\n  ..   socialmedia = col_double(),\n  ..   gender = col_character(),\n  ..   siblings = col_double(),\n  ..   smed.hrs = col_double(),\n  ..   gram.followers = col_double(),\n  ..   fbfriends = col_double(),\n  ..   tvhours = col_double(),\n  ..   haircolor = col_double(),\n  ..   belief.in.god = col_double(),\n  ..   liveoncampus = col_double(),\n  ..   numclasses = col_double(),\n  ..   numclassesremote = col_double(),\n  ..   eatmeat = col_double(),\n  ..   operas = col_double(),\n  ..   cigarettes = col_double(),\n  ..   like.science = col_double(),\n  ..   harrypotter = col_double(),\n  ..   registeredtovote = col_double(),\n  ..   votefor = col_double(),\n  ..   expectedoutcome_1 = col_double(),\n  ..   expectedoutcome_2 = col_double(),\n  ..   expectedoutcome_3 = col_double(),\n  ..   expectedoutcome_4 = col_double(),\n  ..   majordiv = col_double(),\n  ..   hrs.sleep = col_double(),\n  ..   height.unvalidated = col_double(),\n  ..   shootingdrills = col_double(),\n  ..   handedness = col_double()\n  .. )\n\nnames(friends)\n\n\n [1] \"duration\"            \"finished\"            \"whentook\"           \n [4] \"DistributionChannel\" \"socialmedia\"         \"gender\"             \n [7] \"siblings\"            \"smed.hrs\"            \"gram.followers\"     \n[10] \"fbfriends\"           \"tvhours\"             \"haircolor\"          \n[13] \"belief.in.god\"       \"liveoncampus\"        \"numclasses\"         \n[16] \"numclassesremote\"    \"eatmeat\"             \"operas\"             \n[19] \"cigarettes\"          \"like.science\"        \"harrypotter\"        \n[22] \"registeredtovote\"    \"votefor\"             \"expectedoutcome_1\"  \n[25] \"expectedoutcome_2\"   \"expectedoutcome_3\"   \"expectedoutcome_4\"  \n[28] \"majordiv\"            \"hrs.sleep\"           \"height.unvalidated\" \n[31] \"shootingdrills\"      \"handedness\"         \n\nhead(friends)\n\n\n# A tibble: 6 x 32\n  duration finished whentook            DistributionCha… socialmedia\n     <dbl>    <dbl> <dttm>              <chr>                  <dbl>\n1      464        1 2020-08-27 13:05:27 anonymous                  1\n2      332        1 2020-08-27 14:58:31 anonymous                 12\n3      399        1 2020-08-27 15:06:45 anonymous                  1\n4      328        1 2020-08-27 15:26:38 anonymous                 12\n5       37        0 2020-08-28 09:22:43 anonymous                 NA\n6      563        1 2020-08-28 11:24:18 anonymous                  1\n# … with 27 more variables: gender <chr>, siblings <dbl>,\n#   smed.hrs <dbl>, gram.followers <dbl>, fbfriends <dbl>,\n#   tvhours <dbl>, haircolor <dbl>, belief.in.god <dbl>,\n#   liveoncampus <dbl>, numclasses <dbl>, numclassesremote <dbl>,\n#   eatmeat <dbl>, operas <dbl>, cigarettes <dbl>,\n#   like.science <dbl>, harrypotter <dbl>, registeredtovote <dbl>,\n#   votefor <dbl>, expectedoutcome_1 <dbl>, expectedoutcome_2 <dbl>,\n#   expectedoutcome_3 <dbl>, expectedoutcome_4 <dbl>, majordiv <dbl>,\n#   hrs.sleep <dbl>, height.unvalidated <dbl>, shootingdrills <dbl>,\n#   handedness <dbl>\n\nclass(friends$gender)\n\n\n[1] \"character\"\n\nYou can also click on the name friends in the “Environment” pane at the upper right of the RStudio screen; it will open it up in a tab for you to take a look. Does it look like you’d expect? (If you hit the blue “play button” arrow next to the name in that pane, you’ll see something that looks like what you saw when you ran the str() command.)\nCounting\nUse the count() command from library(dplyr) to figure out how many respondents responded anonymously by running it on the DistributionChannel column of friends.\n\n\nfriends %>% count(DistributionChannel)\n\n\n# A tibble: 1 x 2\n  DistributionChannel     n\n  <chr>               <int>\n1 anonymous              84\n\nThen, create a table() of siblings from friends.\n\n\ntable(friends$siblings)\n\n\n\n 0  1  2  3  4  5  8 \n 6 34 24 12  2  4  1 \n\nUse a chain of filter() and count() to count how many classes (numclasses) the friends who have one sibling have. Said another way: filter() to people who have siblings of 1, then count() the number of classes.\n\n\nfriends %>% \n  filter(siblings == 1) %>% \n  count(numclasses)\n\n\n# A tibble: 5 x 2\n  numclasses     n\n       <dbl> <int>\n1          3     3\n2          4    25\n3          5     3\n4          8     1\n5         NA     2\n\nSome plots\nMake a histogram of the duration it took people to finish the survey. Your choice if you use the ggplot() + geom_histogram() from ggplot2 or just hist(). Label your axes either way.\n\n\nggplot(friends, aes(x = duration)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(x = \"Duration (s)\", y = \"Frequency\", title = \"How long people took to respond\")\n\n\n\n\nYou may notice that this looks pretty bad because there’s an outlier. We can exclude them using filter()! Create a new variable called friends2 which filters out any duration over 10000 (hint: you’ll want the < sign, since we’re looking to keep in the new data frame any duration SHORTER than 10000). Then copy the code for the histogram to the space below, and re-run it (make sure to change to friends2, now).\n\n\nfriends2 <- friends %>% filter(duration < 10000)\nggplot(friends2, aes(x = duration)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(x = \"Duration (s)\", y = \"Frequency\", title = \"How long people took to respond\")\n\n\n\n\nNot very normal, huh?\nHave you saved this file? Please do. If you think your code should work, try knitting the document, too. Does it? If there are errors, can you identify them?\nCreate another data-frame variable called friends3, and this time filter out any duration over 600 s—seems like most people should have finished that in 10 minutes, barring them opening it and then getting distracted. Then, make a geom_histogram() of the data. (You can re-use your code, of course—just be sure to call it friends3 in the ggplot() command.) You should find that the data looks MUCH more normally-distributed, right?\n\n\nfriends3 <- friends %>% filter(duration < 600)\nggplot(friends3, aes(x = duration)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(x = \"Duration (s)\", y = \"Frequency\", title = \"How long people took to respond\")\n\n\n\n\nIf we actually dropped those people, that would be dropping seven responses. We won’t right now. It’s not necessarily bad data if someone took two hours to hit submit… but perhaps you’re seeing some of the decisions that we go into in data analysis and the preparation for it.\nBox and violin plots\nFor this last visualization, we’re going to need to tell R that number of siblings is a factor, not a continuous variable. (No-one has 1.5 siblings.) Run the following code to do that. It literally tells R “this is a factor” with the factor() function.\n\n\nfriends$siblings <- factor(friends$siblings)\n\n\n\nUse friends (rather than friends2) for this. Make both a geom_boxplot() and a geom_violin() of the data for gram.followers against siblings. (You can try doing them as two layers on the same plot, but it doesn’t look too good.) Up to you which is x and which is y; this is a case where it doesn’t matter. Do add on a theme, and labs(). Play around with colors if you like—this is a great time to start learning how to do that.\n\n\nggplot(friends, aes(y = siblings, x = gram.followers)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(x = \"Number of instagram followers\", y = \"Number of siblings\")\n\n\n\n\nBased on the plot, do you think that people with more siblings have more of a social media presence? Are there any outliers?\nKnit the document. How’s it looking?\nReal study example\nFiorella & Mayer (2013) hypothesized that students would learn course material better if they thought they were going to later be asked teach the material to the rest of the class. To test this, the researchers divided students into three groups. All groups read a short excerpt about the Doppler effect and were later given a 10-question quiz. The control group studied the excerpt and then immediately took the quiz. The preparation group was instructed that they would later teach the material to a group of students. This group studied the excerpt then immediately took the quiz. Finally, the teaching group was instructed that they would later teach the material to a group of students. This group studied the excerpt, taught it to a group of students, and then took the quiz. Fiorella & Mayer reported the following results:\nGroup\nn\nComprehension score\n\n\n\nM\nSD\nControl\n31\n6.2\n3.3\nPreparation\n32\n7.9*\n2.4\nTeaching\n30\n8.7*\n2.8\n* Significantly different from control group at p < .05\nMake a data.frame() with the data above. Two suggestions: (a) name everything (the data frame variable, the columns) with lower case names without spaces (the group’s names will be in quotes, and can be however you like), and (b) remember that you name each column in the data.\n\n\ngraphdata <- data.frame(group = c(\"Control\", \"Preparation\", \"Teaching\"),\n                        n = c(31, 32, 30),\n                        score = c(6.2, 7.9, 8.7),\n                        sd = c(3.3, 2.4, 2.8))\n\n\n\nUsing the above factor(), make the group column of your new data frame a factor.\n\n\ngraphdata$group <- factor(graphdata$group)\n\n\n\nPlot these results in a bar graph with ggplot2 and using the geom_col() layer. The x in your ggplot() aes() should be the group, and the y should be the comprehension score. Your graph should include a title, labeled axes, and clearly demonstrate the relationship between group and comprehension score. Feel free to add a theme.\n\n\nggplot(graphdata, aes(x = group, y = score)) +\n  geom_col() +\n  labs(x = \"Group\", y = \"Mean Comprehension Score\", title = \"Comprehension Scores by Group\") +\n  theme_bw()\n\n\n\n\nChallenge: Calculate the standard error of the mean based on the above plot and the equation for SEM, which is \\(SEM=\\frac{SD}{\\sqrt{n}}\\)\n\n\ngraphdata$se <- graphdata$sd / sqrt(graphdata$n)\ngraphdata$se\n\n\n[1] 0.5926975 0.4242641 0.5112077\n\nExtension\nIf you’re feeling comfortable with all of this, this exercise is for you.\nThe data lives in the HistData package, which we installed earlier.\n\n\ninstall.packages(\"HistData\")\n\n\n\nLoad this package here, and the data:\n\n\nlibrary(HistData)\ndata(Nightingale)\n\n\n\nThis is the data from Florence Nightingale’s research in the 1850s on causes of death after the Crimean war—which we will discuss in class. You can take a look by running str(Nightingale). (As always, remove the tick marks before pasting that code into the Console.)\nFor the 24 months listed, the data has how many deaths were caused by disease in the column: Nightingale$Disease\nHere’s a ggplot() version of the Nightingale coxcomb plot we saw in class—it’s also on wikipedia, here. For some more context, read this.\n\n\ndeaths <- Nightingale %>% \n  mutate(Month = factor(Month, \n                        levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \n                                   \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))) %>%\n  pivot_longer(cols = c(Disease, Other, Wounds), names_to = \"Cause\", values_to = \"Number\")\nyear1 <- ggplot(deaths %>% slice(1:36)) + \n  geom_bar(aes(x = Month, y = Number, fill = Cause), color = \"grey40\", \n           stat = \"identity\", alpha = .6, width = 1.0, size =.5) + \n  coord_polar() +\n  scale_y_sqrt() + # actually, Nightingale used a square root transformation, which this applies\n  scale_x_discrete(limits = c(\"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\", \n                              \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"), \n                   labels = c(\"Jul-1854\", \"Aug-1854\", \"Sep-1854\", \"Oct-1854\", \n                              \"Nov-1854\", \"Dec-1854\", \"Jan-1855\", \"Feb-1855\", \n                              \"Mar-1855\", \"Apr-1854\", \"May-1854\", \"Jun-1854\")) +\n  geom_abline(intercept = sqrt(1000), slope = 0, linetype = 5) +\n  theme_classic() + \n  scale_fill_discrete(name = \"Cause\", type = c(\"light blue\", \"black\", \"pink\")) +\n  theme(axis.line = element_blank(),  axis.title = element_blank(), \n        axis.ticks = element_blank(), axis.text.y = element_blank(),\n        title = element_text(size = 16))\nyear2 <- ggplot(deaths %>% slice(37:72)) + \n  geom_bar(aes(x = Month, y = Number, fill = Cause), color = \"grey40\", \n           stat = \"identity\", alpha = .6, width = 1.0, size =.5) + \n  coord_polar() +\n  scale_y_sqrt() + # actually, Nightingale used a square root transformation, which this applies\n  scale_x_discrete(limits = c(\"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\", \n                              \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"), \n                   labels = c(\"Jul-1855\", \"Aug-1855\", \"Sep-1855\", \"Oct-1855\", \n                              \"Nov-18554\", \"Dec-1855\", \"Jan-1856\", \"Feb-1856\", \n                              \"Mar-1856\", \"Apr-1855\", \"May-1855\", \"Jun-1855\")) +\n  geom_abline(intercept = sqrt(1000), slope = 0, linetype = 5) +\n  theme_classic() + \n  scale_fill_discrete(name = \"Cause\", type = c(\"light blue\", \"black\", \"pink\")) +\n  theme(axis.line = element_blank(),  axis.title = element_blank(), \n        axis.ticks = element_blank(), axis.text.y = element_blank(),\n        title = element_text(size = 16), legend.position = \"none\")\ngridExtra::grid.arrange(grobs = list(year2, year1), nrow = 1,\n                        top = grid::textGrob(\"Diagram of the Causes of Mortality\\nin the Army in the East\"))\n\n\n\n\nYou don’t have to do that, however. (And we can all acknowledge that hers is more interesting to look at than this version!)\nLabel your axes in all of these plots.\nExtension 1. Create a histogram using the full Nightingale$Disease data. Note that this should only involve the disease data—a histogram shows frequencies of how often you get a certain response. Is the data normally distributed?\n\n\n# in ggplot:\nggplot(data = Nightingale, aes(x = Disease)) +\n  geom_histogram(binwidth = 300) + \n  theme_bw() +\n  labs(x = \"Number of Disease Deaths\", y = \"Frequency\", title = \"Frequency of Disease Deaths\")\n\n\n\n# in base R:\nhist(Nightingale$Disease, \n     xlab = \"Number of Disease Deaths\", ylab = \"Frequency\", main = \"Frequency of Disease Deaths\")\n\n\n\n\n\nNo, not normally distributed!\n\nCreate a scatterplot of deaths from Wounds against those from Disease. To do this, use the geom_point() layer from ggplot2.\n\n\nggplot(data = Nightingale, aes(x = Wounds, y = Disease)) + \n  geom_point(color = \"red\", alpha = .5) + # setting alpha as lower than 1 makes the points partially transparent\n  theme_bw() +\n  labs(x = \"Deaths from Wounds\", y = \"Deaths from Disease\", \n       title = \"Comparison of Deaths from Disease and from Wounds\")\n\n\n\n\nCreate a line graph of Disease over time (use the Date column for time). You should not need to create any new variables. As in all of these, make sure to label all axes. The graph should clearly demonstrate the relationship between Disease and Date.\nYou’ll note that R is actually quite good at handling dates! It knows these dates are class(Nightingale$Date) == \"Date\"!\n\n\n# in ggplot\nggplot(data = Nightingale, aes(x = Date, y = Disease)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  labs(x = \"Date\", y = \"Number of Deaths by Disease\", \n       title = \"Deaths by Disease over Time\")\n\n\n\n\n\n\n\n",
    "preview": "answers/03-lab/lab-3-completed_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-01-29T12:20:30-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "answers/02-lab/",
    "title": "Introduction to R (Lab 02) Exercises, Completed",
    "description": "Completed exercises for the second lab",
    "author": [
      {
        "name": "Justin Dainer-Best",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [
      "rlanguage",
      "comments",
      "dplyr"
    ],
    "contents": "\n\n\n\nThis is an R Markdown document. When you execute code within the notebook, the results appear beneath the code. You can find a completed version of these exercises at https://jdbest.github.io/psychRstats/answers.html\nThis is the set of exercises for you to complete after the first lab tutorial. Please read the text and try the code as it follows!\nIf you would like to refer to the tutorial while also working on this document, there are instructions on the wiki on how to do so.\nThese documents are the second way (after tutorials like what you’ve already completed) that you will interact with data in R Studio during our lab classes. The “markdown” here is similar to what you may have seen on web forums, on Wikipedia, or elsewhere online. It is pretty straightforward, and mostly just involves basic word processing. Fortunately, you can also find some excellent “cheatsheets” by searching online for an R Markdown cheatsheet, or start here on the R studio website.\nWant to learn more about Markdown? I recommend spending 15 minutes completing this tutorial: https://commonmark.org/help/tutorial/\nYou’ll also see that things like asterisks make font italics (two make font bold) in the knitted output—but we’ll talk more about that, below. Similarly, when I put text between tick marks like this, it usually means that it’s code!\nHow to run code in these documents\nYou can try executing the “chunk” below by clicking the Run button within the chunk, or by placing your cursor inside it and pressing (on a Mac) Cmd+Shift+Enter. (On a PC, try instead pressing Ctrl+Shift+Enter. Many shortcuts in R will use the Command key on a Mac and the Control key on a PC.) If you only want to execute (run) one line at a time, you can always just hit Cmd+Enter (Ctrl+Enter) while the cursor is on that line—or just a selection of text.\n\n\n# Load the relevant packages\nlibrary(tidyverse)\n\n\n\nThe code that you just ran loads the tidyverse package that we installed last lab. You need to load some packages every time you load RStudio; this is one of them.\nWhen you ran that line, it may have “minimized” the Console window and made this pane larger. That’s fine! It’s still running the code in the Console, though. If you click on the word Console below, you’ll see that it loaded several packages.\nLoading a package lets you use the functions (commands) that the package has. Functions like the chain operator %>% or select() that we talked about in the tutorial come from a package called dplyr; that package is loaded by running the above command, but we could also load it explicitly with:\n\n\nlibrary(dplyr)\n\n\n\nYou can add a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I. (Ctrl+Alt+I on a PC.) Hit enter twice (from the end of this line) and give it a try! Write something like 5+5 in the code chunk you create, and try running it. Make sure the code works!\nKnitting\nIn a code chunk, any code you write will “run” in the console when you run it, or when you Knit the document. Knitting an R Markdown document will create an HTML file (a website) with all of your code. Try hitting the Knit button (it has a ball of yarn next to it) at the top of this pane. You should get a browser that pops up with the whole document. (You can close it to return here.)\nIf there’s an error, that’s okay—it might be because your working directory isn’t quite right. Try to fix it!\nThese HTML files contain all of the code and its output, and are saved in the same folder as the .Rmd (R Markdown) file. Again: click the Knit button or press Cmd+Shift+K [Ctrl+Shift+K] to see the HTML file.\nThese HTML files are useful for sharing with others to show what you’ve done!\nA general rule about knitting R Markdown: do it early and often. Much better to find errors right away, rather than figuring them out when you thought you were done.\nIf you have any questions at this point, take some time to make sense of everything!\nImporting data\nYou can import data from the File menu in RStudio. However, most of the time you’ll want to do so using code.\nWhen you ran the line that downloaded this file, it also downloaded a CSV (comma-separated value) file with data about penguins. You can read more about that data here. (That will be a link when you knit this—either knit it and click, or copy and paste the URL into your web browser if you’d like to read it.) The data is from Horst, Hill, & Gorman (2020).\nYou can use the function read_csv() to, well, read a CSV file. Try running the following code. (How? Scroll back up and read the section “How to run code in these documents”.)\n\n\nread_csv(\"penguins.csv\")\n\n\n# A tibble: 344 x 8\n   species island bill_length_mm bill_depth_mm flipper_length_…\n   <chr>   <chr>           <dbl>         <dbl>            <dbl>\n 1 Adelie  Torge…           39.1          18.7              181\n 2 Adelie  Torge…           39.5          17.4              186\n 3 Adelie  Torge…           40.3          18                195\n 4 Adelie  Torge…           NA            NA                 NA\n 5 Adelie  Torge…           36.7          19.3              193\n 6 Adelie  Torge…           39.3          20.6              190\n 7 Adelie  Torge…           38.9          17.8              181\n 8 Adelie  Torge…           39.2          19.6              195\n 9 Adelie  Torge…           34.1          18.1              193\n10 Adelie  Torge…           42            20.2              190\n# … with 334 more rows, and 3 more variables: body_mass_g <dbl>,\n#   sex <chr>, year <dbl>\n\nIt should just work—this is the great thing about having your working directory set! (If it doesn’t work on its own, try adding 02-lab/ before the name \"penguins.csv\", and again in the code chunk below. Because of where it looks for the file when knitting vs. running interactively, you may well find that it needs no directory to knit, but needs the specific directory included to run right now.) You might see some language about how it’s been “Parsed with column specification”—great! It’s telling you the types (classes) of variables.\nNow, you may remember from the tutorial, that if it’s printing to the screen, it’s not actually saving this data. To do that, we need to assign it to a variable. Run this code:\n\n\npenguins <- read_csv(\"penguins.csv\")\n\n\n\nYou should see the variable penguins appear in the Environment pane. (If you don’t see the Environment pane, it’s one of the tabs in the top right!) It’ll tell you that there are 344 observations of 8 variables—i.e., the data frame has 344 rows and 8 columns.\n(Aside: You don’t have to use code to import data. You could also [a] click on the “Import Dataset” button in the Environment pane, or [b] in the Files pane, click on the filename of the file when you’re in the right folder.)\nExercises\nWhat class is this variable penguins? In the code chunk below, use the class() function to find out! (It should give you more than one answer, including that it’s a “tbl” [table] and a data.frame)\n\n\nclass(penguins)\n\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nUsing both the select() function from the dplyr package and the $ operator (which is the default in R), print the column species from penguins. (Do one on the first line and the other on the second.)\nRemember that you should use the chain %>% with select(). Feel free to refer back to your notes or the tutorial!\n\n\n# with select()\npenguins %>% select(species)\n\n\n# A tibble: 344 x 1\n   species\n   <chr>  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\n# or:\nselect(penguins, species)\n\n\n# A tibble: 344 x 1\n   species\n   <chr>  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# … with 334 more rows\n\n# with the $\npenguins$species\n\n\n  [1] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n  [6] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [11] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [16] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [21] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [26] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [31] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [36] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [41] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [46] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [51] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [56] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [61] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [66] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [71] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [76] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [81] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [86] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [91] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n [96] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[101] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[106] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[111] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[116] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[121] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[126] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[131] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[136] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[141] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[146] \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"    \"Adelie\"   \n[151] \"Adelie\"    \"Adelie\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[156] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[161] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[166] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[171] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[176] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[181] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[186] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[191] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[196] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[201] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[206] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[211] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[216] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[221] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[226] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[231] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[236] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[241] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[246] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[251] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[256] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[261] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[266] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[271] \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"    \"Gentoo\"   \n[276] \"Gentoo\"    \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[281] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[286] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[291] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[296] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[301] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[306] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[311] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[316] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[321] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[326] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[331] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[336] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n[341] \"Chinstrap\" \"Chinstrap\" \"Chinstrap\" \"Chinstrap\"\n\nUsing the slice() function from dplyr, print the 20th row of penguins.\n\n\npenguins %>% slice(20)\n\n\n# A tibble: 1 x 8\n  species island bill_length_mm bill_depth_mm flipper_length_…\n  <chr>   <chr>           <dbl>         <dbl>            <dbl>\n1 Adelie  Torge…             46          21.5              194\n# … with 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n\nOkay, now combine them! Use a chain %>% between the commands. First, select() the column species from penguins and second, slice() the 20th row.\n\n\npenguins %>% \n  select(species) %>%\n  slice(20)\n\n\n# A tibble: 1 x 1\n  species\n  <chr>  \n1 Adelie \n\nIn words, describe what is going on in #4. (Just type it in the space below.)\n\nWe take the data frame “penguins” and select the column “species” from it, and then we slice the 20th row – so we get the value of “Adelie” (which is the 20th value in that row.)\n\nTry knitting the document. Does everything work? If there’s an error, is it one you can address? Try to fix it.\nYou can also use square brackets [like these] after the name of a variable to subset part of that variable, as you learned in the tutorial. With a data frame, if you put any number inside the brackets, it will select the column that corresponds to that number. Use this method to select the 4th column of penguins in the code chunk below.\nTechnically, there’s a comma that comes before the 4, inside the brackets. (Nothing comes before the comma.) Try that out—you should get the same result.\n\n\npenguins[4]\n\n\n# A tibble: 344 x 1\n   bill_depth_mm\n           <dbl>\n 1          18.7\n 2          17.4\n 3          18  \n 4          NA  \n 5          19.3\n 6          20.6\n 7          17.8\n 8          19.6\n 9          18.1\n10          20.2\n# … with 334 more rows\n\npenguins[,4] # same thing\n\n\n# A tibble: 344 x 1\n   bill_depth_mm\n           <dbl>\n 1          18.7\n 2          17.4\n 3          18  \n 4          NA  \n 5          19.3\n 6          20.6\n 7          17.8\n 8          19.6\n 9          18.1\n10          20.2\n# … with 334 more rows\n\nidentical(penguins[4], penguins[,4]) # tests if they're identical---TRUE\n\n\n[1] TRUE\n\nIf you put a comma after the number in the square brackets, it selects the row from a data frame. (e.g., dataframe[number,]). Try this with the penguins data: select the fifth row below.\n\n\npenguins[5,]\n\n\n# A tibble: 1 x 8\n  species island bill_length_mm bill_depth_mm flipper_length_…\n  <chr>   <chr>           <dbl>         <dbl>            <dbl>\n1 Adelie  Torge…           36.7          19.3              193\n# … with 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n\nOkay, now combine them: select the fifth row of the fourth column in penguins by selecting [5,4] after the variable penguins.\n\n\npenguins[5,4]\n\n\n# A tibble: 1 x 1\n  bill_depth_mm\n          <dbl>\n1          19.3\n\nThis is actually similar to how Excel thinks of cells: it’s called “row, column” notation.\nNext step: Filtering\nWhat about filtering data? Imagine that we were only interested in Chinstrap penguins (not Adelie). Well, we can filter those out with the filter() function from the dplyr package.\nYou can click on the Help pane at the lower right of the RStudio window, or type ?filter in the Console, to find out more about filter. Essentially, it does a logical check for some condition that you provide. What’s that mean?\nWell, filter() wants something that results in either TRUE or FALSE—a logical (or Boolean) response. Let’s check that out: just run the following code:\n\n\npenguins$species == \"Chinstrap\"\n\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [11] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [21] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [31] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [41] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [51] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [71] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [81] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [91] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[101] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[111] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[131] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[141] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[151] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[161] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[171] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[191] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[201] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[211] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[221] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[231] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[251] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[261] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[271] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[281]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[291]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[311]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[321]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[331]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[341]  TRUE  TRUE  TRUE  TRUE\n\nWhat you’ll get is a series of TRUE and FALSE values, as R tests every one of the 344 rows of the species column against the name “Chinstrap”. Looks like the Chinstrap penguins are the last of the set.\nThe double equal sign can be read as “is this equal?” or a test for equivalence. If we put that inside of a filter() function, we’ll get only the rows where it’s true.\nSo: filter() works like select() or slice(). Name the variable, use the %>% chain, and then use the filter() function with this code inside the parentheses: species == \"Chinstrap\". Give that a try below. (Remember not to include the tick marks ` themselves in the code you write.)\n\n\npenguins %>%\n  filter(species == \"Chinstrap\")\n\n\n# A tibble: 68 x 8\n   species island bill_length_mm bill_depth_mm flipper_length_…\n   <chr>   <chr>           <dbl>         <dbl>            <dbl>\n 1 Chinst… Dream            46.5          17.9              192\n 2 Chinst… Dream            50            19.5              196\n 3 Chinst… Dream            51.3          19.2              193\n 4 Chinst… Dream            45.4          18.7              188\n 5 Chinst… Dream            52.7          19.8              197\n 6 Chinst… Dream            45.2          17.8              198\n 7 Chinst… Dream            46.1          18.2              178\n 8 Chinst… Dream            51.3          18.2              197\n 9 Chinst… Dream            46            18.9              195\n10 Chinst… Dream            51.3          19.9              198\n# … with 58 more rows, and 3 more variables: body_mass_g <dbl>,\n#   sex <chr>, year <dbl>\n\nYou should get 68 rows—all of which are of the Chinstrap species.\nYou’ll note that I didn’t need to specify the name of the data frame again inside of the filter() function. In fact, you should not name the data frame again.\nLast two pieces: use filter() again to only select rows from penguins where the bill length in millimeters (column name: bill_length_mm) is more than 40.\n\n\npenguins %>%\n  filter(bill_length_mm > 40)\n\n\n# A tibble: 242 x 8\n   species island bill_length_mm bill_depth_mm flipper_length_…\n   <chr>   <chr>           <dbl>         <dbl>            <dbl>\n 1 Adelie  Torge…           40.3          18                195\n 2 Adelie  Torge…           42            20.2              190\n 3 Adelie  Torge…           41.1          17.6              182\n 4 Adelie  Torge…           42.5          20.7              197\n 5 Adelie  Torge…           46            21.5              194\n 6 Adelie  Biscoe           40.6          18.6              183\n 7 Adelie  Biscoe           40.5          17.9              187\n 8 Adelie  Biscoe           40.5          18.9              180\n 9 Adelie  Dream            40.9          18.9              184\n10 Adelie  Dream            42.2          18.5              180\n# … with 232 more rows, and 3 more variables: body_mass_g <dbl>,\n#   sex <chr>, year <dbl>\n\nThen, copy that code below—and after the 40, add a comma, and a second thing to filter by. You want to filter() only rows from penguins where (again) the bill length in millimeters (column name: bill_length_mm) is more than 40, and where the species is Adelie.\n\n\npenguins %>%\n  filter(bill_length_mm > 40, \n         species == \"Adelie\")\n\n\n# A tibble: 51 x 8\n   species island bill_length_mm bill_depth_mm flipper_length_…\n   <chr>   <chr>           <dbl>         <dbl>            <dbl>\n 1 Adelie  Torge…           40.3          18                195\n 2 Adelie  Torge…           42            20.2              190\n 3 Adelie  Torge…           41.1          17.6              182\n 4 Adelie  Torge…           42.5          20.7              197\n 5 Adelie  Torge…           46            21.5              194\n 6 Adelie  Biscoe           40.6          18.6              183\n 7 Adelie  Biscoe           40.5          17.9              187\n 8 Adelie  Biscoe           40.5          18.9              180\n 9 Adelie  Dream            40.9          18.9              184\n10 Adelie  Dream            42.2          18.5              180\n# … with 41 more rows, and 3 more variables: body_mass_g <dbl>,\n#   sex <chr>, year <dbl>\n\nWhen that’s all done, Knit the document again. If there are errors, try to solve them.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-29T12:20:48-05:00",
    "input_file": {}
  }
]
